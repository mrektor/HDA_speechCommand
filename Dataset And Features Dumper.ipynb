{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "## save variables\n",
    "import pickle\n",
    "## folder names\n",
    "from glob import glob\n",
    "## wav import\n",
    "from scipy.io import wavfile \n",
    "## standard libraries\n",
    "import numpy as np\n",
    "## MFCC\n",
    "#!{sys.executable} -m pip install msgpack --user\n",
    "#!{sys.executable} -m pip install python_speech_features --user\n",
    "\n",
    "from python_speech_features import mfcc\n",
    "from python_speech_features import logfbank\n",
    "from python_speech_features import delta\n",
    "\n",
    "import random as rnd\n",
    "import os.path\n",
    "import tarfile\n",
    "\n",
    "from six.moves import urllib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "%matplotlib notebook\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "\n",
    "\n",
    "#!{sys.executable} -m pip install opencv-python --user\n",
    "#!{sys.executable} -m pip install opencv-contrib-python --user\n",
    "#import cv2\n",
    "#garbage collector\n",
    "import gc\n",
    "#OS detection\n",
    "import platform\n",
    "\n",
    "#!{sys.executable} -m pip install librosa --user\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shift signal of #value samples padding with zero to return the same dimension\n",
    "def shiftVec(signal, value):\n",
    "    initial_length = signal.shape[0]\n",
    "    padded = np.pad(signal, (abs(value),abs(value)), 'constant', constant_values=0)\n",
    "    signal = padded[abs(value)-value:abs(value)+initial_length-value]\n",
    "    return signal\n",
    "#return a random noise of nSample\n",
    "def noiseSelector(noise, nSample):\n",
    "    length = len(noise)\n",
    "    choice = rnd.randint(0, length-1)\n",
    "    key = list(noise.keys())[choice]\n",
    "    start = rnd.randint(0, noise[key].shape[0]-nSample-1)\n",
    "    return noise[key][start:start+nSample]\n",
    "\n",
    "## Return the word between two string starting from left\n",
    "def find_between( s, first, last ):\n",
    "    try:\n",
    "        start = s.index( first ) + len( first )\n",
    "        end = s.index( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\"\n",
    "\n",
    "# stretching the sound\n",
    "def stretch(data, rate=1):\n",
    "    input_length = data.shape[0]\n",
    "    data = librosa.effects.time_stretch(data, rate)\n",
    "    if len(data)>input_length:\n",
    "        data = data[round((data.shape[0]-input_length)/2):round((data.shape[0]+input_length)/2)]\n",
    "    else:\n",
    "        data = np.pad(data, (0, max(0, input_length - len(data))), \"constant\", constant_values=0)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make variables directory if not present\n",
    "dest_directory = 'variables/'\n",
    "if not os.path.exists(dest_directory):\n",
    "      os.makedirs(dest_directory)\n",
    "\n",
    "        #data url from which download the dataset      \n",
    "data_url = 'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
    "\n",
    "#make dataset directory if not present\n",
    "dest_directory = 'dataset/'\n",
    "if not os.path.exists(dest_directory):\n",
    "      os.makedirs(dest_directory)\n",
    "\n",
    "#select the last part of the dataurl (the file name)      \n",
    "filename = data_url.split('/')[-1]\n",
    "filepath = os.path.join(dest_directory, filename)\n",
    "\n",
    "#program the download and extraction if the file doesn't exists\n",
    "if not os.path.exists(filepath):\n",
    "    def progress(count, block_size, total_size):\n",
    "        sys.stdout.write(\n",
    "            '\\r>> Downloading %s %.1f%%' %\n",
    "            (filename, float(count * block_size) / float(total_size) * 100.0))\n",
    "        sys.stdout.flush()\n",
    "    try:\n",
    "        filepath, _ = urllib.request.urlretrieve(data_url, filepath, progress)\n",
    "    except:\n",
    "        print(Error)\n",
    "        \n",
    "    tarfile.open(filepath, 'r:gz').extractall(dest_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreKey = ['yes', \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", \"zero\",\n",
    "           \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
    "sampleRate = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import re\n",
    "def which_set(filename, validation_percentage, testing_percentage, totClass):\n",
    "    \"\"\"Determines which data partition the file should belong to.\n",
    "    We want to keep files in the same training, validation, or testing sets even\n",
    "    if new ones are added over time. This makes it less likely that testing\n",
    "    samples will accidentally be reused in training when long runs are restarted\n",
    "    for example. To keep this stability, a hash of the filename is taken and used\n",
    "    to determine which set it should belong to. This determination only depends on\n",
    "    the name and the set proportions, so it won't change as other files are added.\n",
    "    It's also useful to associate particular files as related (for example words\n",
    "    spoken by the same person), so anything after '_nohash_' in a filename is\n",
    "    ignored for set determination. This ensures that 'bobby_nohash_0.wav' and\n",
    "    'bobby_nohash_1.wav' are always in the same set, for example.\n",
    "    Args:\n",
    "    filename: File path of the data sample.\n",
    "    validation_percentage: How much of the data set to use for validation.\n",
    "    testing_percentage: How much of the data set to use for testing.\n",
    "    Returns:\n",
    "    String, one of 'training', 'validation', or 'testing'.\n",
    "    \"\"\"\n",
    "    base_name = os.path.basename(filename)\n",
    "    # We want to ignore anything after '_nohash_' in the file name when\n",
    "    # deciding which set to put a wav in, so the data set creator has a way of\n",
    "    # grouping wavs that are close variations of each other.\n",
    "    hash_name = re.sub(r'_nohash_.*$', '', base_name)\n",
    "    # This looks a bit magical, but we need to decide whether this file should\n",
    "    # go into the training, testing, or validation sets, and we want to keep\n",
    "    # existing files in the same set even if more files are subsequently\n",
    "    # added.\n",
    "    # To do that, we need a stable way of deciding based on just the file name\n",
    "    # itself, so we do a hash of that and then use that to generate a\n",
    "    # probability value that we use to assign it.\n",
    "    hash_name_hashed = hashlib.sha1(hash_name.encode('utf-8')).hexdigest()\n",
    "    percentage_hash = ((int(hash_name_hashed, 16) %\n",
    "                      (totClass + 1)) *\n",
    "                     (100.0 / totClass))\n",
    "    if percentage_hash < validation_percentage:\n",
    "        result = 'validation'\n",
    "    elif percentage_hash < (testing_percentage + validation_percentage):\n",
    "        result = 'testing'\n",
    "    else:\n",
    "        result = 'training'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#control if the raw data are saved with pickle\n",
    "#if true they will be loaded in rawDict\n",
    "reDo = False\n",
    "pathName = {}\n",
    "if os.path.exists('variables/rawDict.pkl') and not reDo:\n",
    "    print('rawDict found')\n",
    "    with open('variables/rawDict.pkl', 'rb') as f:  \n",
    "        rawDict = pickle.load(f)\n",
    "    with open('variables/pathName.pkl', 'rb') as f:  \n",
    "        pathName = pickle.load(f)\n",
    "    print('rawDict loaded')\n",
    "\n",
    "#else they will be loaded from wav files\n",
    "else:\n",
    "    print('Creating rawDict')\n",
    "    folders = glob(\"dataset/*/\")\n",
    "    folders.remove('dataset\\\\_background_noise_\\\\')\n",
    "    print('SIGNALS')\n",
    "    rawDict = {}\n",
    "    for key in folders:\n",
    "        print('Processing ', key)\n",
    "        dictKey = find_between( key, '\\\\', '\\\\' )\n",
    "        tmpFiles = glob(key+'*')\n",
    "        array = []\n",
    "        pathList = []\n",
    "        for file in tmpFiles:\n",
    "            pathList.append(file)\n",
    "            tmp = wavfile.read(file)[1].copy()\n",
    "            tmp.resize(16000, refcheck=False)\n",
    "            array.append(tmp)\n",
    "        rawDict[dictKey] = np.array(array)\n",
    "        pathName[dictKey] = pathList\n",
    "    #and saved with pickle\n",
    "    with open('variables/rawDict.pkl', 'wb') as f:  \n",
    "        pickle.dump(rawDict, f)\n",
    "    with open('variables/pathName.pkl', 'wb') as f:  \n",
    "        pickle.dump(pathName, f)\n",
    "    print('rawDict created and saved to variables/rawDict.pkl')\n",
    "reDo = True\n",
    "#the same with noise signals\n",
    "if not os.path.exists('variables/noiseDict.pkl') or reDo:    \n",
    "    print('\\nNOISE')\n",
    "    noiseDict = {}\n",
    "    folders = glob('dataset/_background_noise_/*.wav')\n",
    "    for key in folders:\n",
    "        print('Processing ', key)\n",
    "        noiseDict[key[key.rindex('/')+1:len(key)]] = wavfile.read(key)[1]\n",
    "    with open('variables/noiseDict.pkl', 'wb') as f:  \n",
    "        pickle.dump(noiseDict, f) \n",
    "    print('noiseDict created and saved to variables/noiseDict.pkl')\n",
    "else:\n",
    "    print('noiseDict found')\n",
    "    with open('variables/noiseDict.pkl', 'rb') as f:  \n",
    "        noiseDict = pickle.load(f)\n",
    "    print('noiseDict loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "shift_percentage = 0.1\n",
    "time_shift_max = 100 #[ms]\n",
    "sample_shift_max = round(time_shift_max / 1000 * sampleRate)\n",
    "\n",
    "for key in rawDict:\n",
    "    length = rawDict[key].shape[0]\n",
    "    toShift = round(length*shift_percentage)\n",
    "    for i in rnd.sample(range(length),toShift):\n",
    "        shift = rnd.randint(-sample_shift_max, sample_shift_max)\n",
    "        rawDict[key][i] = shiftVec(rawDict[key][i],shift)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "noise_percentage = 0.1 \n",
    "max_noise = .5\n",
    "for key in rawDict:\n",
    "    length = rawDict[key].shape[0]\n",
    "    toNoise = round(length*noise_percentage)\n",
    "    for i in rnd.sample(range(length),toNoise):\n",
    "        noise = noiseSelector(noiseDict, 16000)\n",
    "        rawDict[key][i] += np.array(np.round(np.random.uniform(high = max_noise) * noise),dtype='int16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silence creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'silence'\n",
    "silence_percentage = 0.05\n",
    "max_noise_sound = 0.5\n",
    "#silence_max = round(tot_samples*silence_percentage)\n",
    "silence_max = 5000\n",
    "values = []\n",
    "for i in range(silence_max):\n",
    "    noise = noiseSelector(noiseDict, 16000)\n",
    "    sig = noise * np.random.uniform(high = max_noise_sound)\n",
    "    values.append(sig)\n",
    "values = np.array(values)\n",
    "rawDict[key]=values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainDict = {}\n",
    "validationDict = {}\n",
    "testDict = {}\n",
    "\n",
    "validation_percentage = 15\n",
    "testing_percentage = 15\n",
    "\n",
    "for key in rawDict:\n",
    "    if key == 'silence':\n",
    "        #already random so \n",
    "        trainDict[key]= rawDict[key][0:round(silence_max*(1-validation_percentage/100-testing_percentage/100))]\n",
    "        validationDict[key] = rawDict[key][round(silence_max*(1-validation_percentage/100-testing_percentage/100)):round(silence_max*(1-testing_percentage/100))]\n",
    "        testDict[key] = rawDict[key][round(silence_max*(1-testing_percentage/100)):silence_max]\n",
    "    else:\n",
    "        testTemp = []\n",
    "        trainTemp = []\n",
    "        validTemp = []\n",
    "        for count, sample in enumerate(rawDict[key]):\n",
    "            assign = which_set(pathName[key][count], validation_percentage, testing_percentage, 2**27 - 1)\n",
    "            if assign == 'testing':\n",
    "                testTemp.append(sample)\n",
    "            elif assign == 'training':\n",
    "                trainTemp.append(sample)\n",
    "            elif assign == 'validation':\n",
    "                validTemp.append(sample)\n",
    "        trainDict[key]= np.array(trainTemp)\n",
    "        validationDict[key] = np.array(validTemp)\n",
    "        testDict[key] = np.array(testTemp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "Only on core keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rnd.seed(1)\n",
    "#small shift\n",
    "time_shift_max = 50 #[ms]\n",
    "sample_shift_max = round(time_shift_max / 1000 * sampleRate)\n",
    "\n",
    "#small noise perturbation\n",
    "max_noise = .1\n",
    "\n",
    "tot_samples = 0\n",
    "augmented_percentage = 0.3\n",
    "\n",
    "for key in coreKey:\n",
    "    print(key)\n",
    "    length = trainDict[key].shape[0]\n",
    "    tot_samples+=length\n",
    "    toAugment = round(length*augmented_percentage)\n",
    "    new_bunch_of_samples = []\n",
    "    for i in rnd.sample(range(length),toAugment):\n",
    "        shift = rnd.randint(-sample_shift_max, sample_shift_max)\n",
    "        new_sample = shiftVec(trainDict[key][i],shift)     \n",
    "        noise = noiseSelector(noiseDict, 16000)\n",
    "        new_sample += np.array(np.round(np.random.uniform(high = max_noise) * noise),dtype='int16')\n",
    "        new_sample = np.array(stretch(np.array(new_sample,dtype='float32') , np.random.uniform(low = 0.8, high = 1.2)), dtype='int16')\n",
    "        new_bunch_of_samples.append(new_sample)\n",
    "    new_bunch_of_samples = np.array(new_bunch_of_samples)\n",
    "    trainDict[key] = np.vstack((trainDict[key], new_bunch_of_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list with mfcc parameters in order: [numcep, nfilt, winlen, winstep]+\n",
    "values = [[14,26,0.025,0.01,512]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{verbatim}\n",
    "Parameter         Description\n",
    "signal \t          the audio signal from which to compute features. Should be an N*1 array\n",
    "samplerate \t      the samplerate of the signal we are working with.\n",
    "winlen \t          the length of the analysis window in seconds. Default is 0.025s (25 milliseconds)\n",
    "winstep \t      the step between successive windows in seconds. Default is 0.01s (10 milliseconds)\n",
    "numcep \t          the number of cepstrum to return, default 13\n",
    "nfilt \t          the number of filters in the filterbank, default 26.\n",
    "nfft \t          the FFT size. Default is 512\n",
    "lowfreq \t      lowest band edge of mel filters. In Hz, default is 0\n",
    "highfreq \t      highest band edge of mel filters. In Hz, default is samplerate/2\\\\\n",
    "preemph \t      apply preemphasis filter with preemph as coefficient. 0 is no filter. Default is 0.97\\\\\n",
    "ceplifter \t      apply a lifter to final cepstral coefficients. 0 is no lifter. Default is 22\\\\\n",
    "appendEnergy \t  if this is true, the zeroth cepstral coefficient is replaced with the log of the total frame energy.\\\\\n",
    "returns \t      A numpy array of size (NUMFRAMES by numcep) containing features. Each row holds 1 feature vector.\\\\\n",
    "\\end{verbatim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMFCC(data, output, i, sampleRate = 16000):\n",
    "    for sig in data:\n",
    "        mfcc_feat = mfcc(sig,sampleRate, preemph = 0.97, numcep = i[0], nfilt = i[1], winlen = i[2], winstep = i[3], nfft = i[4], lowfreq=100, highfreq=4000)\n",
    "        output.append(mfcc_feat)\n",
    "def computeLogF(data, output, i, sampleRate = 16000):\n",
    "    for sig in data:\n",
    "        lfilt_feat = logfbank(sig,sampleRate, preemph = 0.97, nfilt = i[1], winlen = i[2], winstep = i[3], nfft = i[4], lowfreq=100, highfreq=4000)\n",
    "        output.append(lfilt_feat)\n",
    "def computeDelta(mfccValues, N = 2):\n",
    "    temp = []\n",
    "    for count, _ in enumerate(mfccValues):\n",
    "        delt = delta(mfccValues[count], N)\n",
    "        temp.append(delt)\n",
    "    return temp        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with open('variables/mfccValues.pkl', 'wb') as f:  \n",
    "    pickle.dump(values, f)\n",
    "\n",
    "def getName(index):\n",
    "    if index == 0:\n",
    "        return 'Train'\n",
    "    elif index == 1:\n",
    "        return 'Test'\n",
    "    elif index == 2:\n",
    "        return 'Validation'\n",
    "    \n",
    "#look for the already computed mfcc\n",
    "dictionaries = [trainDict, testDict, validationDict]\n",
    "for count, i in enumerate(values):\n",
    "    for index, dictionary in enumerate(dictionaries):\n",
    "        print('\\n#####Coumputing '+getName(index)+ ' Set#####')\n",
    "        nameMFCC = 'variables/mfccDict'+getName(index)+'[nC='+str(i[0])+' wL='+str(i[2])+' wS='+str(i[3])+'].pkl'\n",
    "        nameDelta = 'variables/mfccDictDD'+getName(index)+'[nC='+str(i[0])+' wL='+str(i[2])+' wS='+str(i[3])+'].pkl'\n",
    "        mfccDict = {}\n",
    "        for countKey, key in enumerate(dictionary):\n",
    "            print('Processing ',key, \" (\", countKey+1, \"/\", len(dictionary),\")\" )\n",
    "            array = []\n",
    "            computeMFCC(dictionary[key], array, i)            \n",
    "            mfccDict[key] = np.array(array)\n",
    "        \n",
    "        with open(nameMFCC, 'wb') as f:  \n",
    "            pickle.dump(mfccDict, f)\n",
    "            \n",
    "        print(\"\\n  Processing delta and delta-delta\")\n",
    "        for countKey, key in enumerate(mfccDict):\n",
    "            print('Processing ',key, \" (\", countKey+1, \"/\", len(mfccDict),\")\" )\n",
    "            delt = np.array(computeDelta(mfccDict[key]))\n",
    "            deltdelt = np.array(computeDelta(delt))\n",
    "            mfccDict[key] = np.stack([mfccDict[key],delt,deltdelt], axis = -1)\n",
    "        \n",
    "        with open(nameDelta, 'wb') as f:  \n",
    "            pickle.dump(mfccDict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogFilter with delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for count, i in enumerate(values):\n",
    "    for index, dictionary in enumerate(dictionaries):\n",
    "        print('\\n#####Coumputing '+getName(index)+ ' Set#####')\n",
    "        nameLog = 'variables/logfiltDict'+getName(index)+'[nF='+str(i[1])+' wL='+str(i[2])+' wS='+str(i[3])+'].pkl'\n",
    "        nameLogDD = 'variables/logfiltDictDD'+getName(index)+'[nF='+str(i[1])+' wL='+str(i[2])+' wS='+str(i[3])+'].pkl'\n",
    "        logFDict = {}\n",
    "        for countKey, key in enumerate(dictionary):\n",
    "            print('Processing ',key, \" (\", countKey+1, \"/\", len(rawDict),\")\" )\n",
    "            array = []\n",
    "            computeLogF(dictionary[key], array, i)            \n",
    "            logFDict[key] = np.array(array)\n",
    "        \n",
    "        with open(nameLog, 'wb') as f:  \n",
    "            pickle.dump(logFDict, f)\n",
    "            \n",
    "        print(\"\\n  Processing delta and delta-delta\")\n",
    "        \n",
    "        for countKey, key in enumerate(logFDict):\n",
    "            print('Processing ',key, \" (\", countKey+1, \"/\", len(logFDict),\")\" )\n",
    "            delt = np.array(computeDelta(logFDict[key]))\n",
    "            deltdelt = np.array(computeDelta(delt))\n",
    "            logFDict[key] = np.stack([logFDict[key],delt,deltdelt], axis = -1)\n",
    "        \n",
    "        with open(nameLogDD, 'wb') as f:  \n",
    "            pickle.dump(logFDict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yurin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "def getName(index):\n",
    "    if index == 0:\n",
    "        return 'Train'\n",
    "    elif index == 1:\n",
    "        return 'Test'\n",
    "    elif index == 2:\n",
    "        return 'Validation'\n",
    "    \n",
    "import pickle\n",
    "from dependencies import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded variables/mfccDictDDTrain[nC=14 wL=0.025 wS=0.01].pkl\n",
      "Loaded variables/mfccDictDDTest[nC=14 wL=0.025 wS=0.01].pkl\n",
      "Loaded variables/mfccDictDDValidation[nC=14 wL=0.025 wS=0.01].pkl\n"
     ]
    }
   ],
   "source": [
    "#choose which dictionary to use\n",
    "choice =      'mfcc'# 'logfilter' #\n",
    "useDelta =  True\n",
    "\n",
    "data = {}\n",
    "\n",
    "#retrieving of used values for the computation of mfcc\n",
    "with open('variables/mfccValues.pkl', 'rb') as f:  \n",
    "    values = pickle.load(f)\n",
    "    \n",
    "selected = 0\n",
    "\n",
    "if choice == 'mfcc':\n",
    "    for index in range(3):\n",
    "        #name format of the selected data\n",
    "        if useDelta:\n",
    "            name = 'variables/mfccDictDD'+getName(index)+'[nC='+str(values[selected][0])+' wL='+str(values[selected][2])+' wS='+str(values[selected][3])+'].pkl'\n",
    "        else:\n",
    "            name = 'variables/mfccDict'+getName(index)+'[nC='+str(values[selected][0])+' wL='+str(values[selected][2])+' wS='+str(values[selected][3])+'].pkl'\n",
    "        #loading in usedDict of the mfcc dict\n",
    "        with open(name, 'rb') as f: \n",
    "            data[getName(index)] = pickle.load(f)\n",
    "        print('Loaded '+name)\n",
    "\n",
    "elif choice == 'logfilter':\n",
    "    for index in range(3):\n",
    "        #name format of the selected data\n",
    "        if useDelta:\n",
    "            name = 'variables/logfiltDictDD'+getName(index)+'[nF='+str(values[selected][1])+' wL='+str(values[selected][2])+' wS='+str(values[selected][3])+'].pkl'\n",
    "        else:\n",
    "            name = 'variables/logfiltDict'+getName(index)+'[nF='+str(values[selected][1])+' wL='+str(values[selected][2])+' wS='+str(values[selected][3])+'].pkl'\n",
    "        #saving in usedDict of the logfilter dict\n",
    "        with open(name, 'rb') as f:  \n",
    "            data[getName(index)] = pickle.load(f)\n",
    "        print('Loaded '+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#core words of the dataset\n",
    "coreKey = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", \"zero\",\n",
    "           \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
    "\n",
    "#split of the core set\n",
    "numbers = ['one', 'two', 'three','four','five','six','seven','eight','nine', \"zero\"]\n",
    "\n",
    "words = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\"]\n",
    "\n",
    "#selecting the subset of words to predict\n",
    "usedLabels = words\n",
    "\n",
    "#usedLabels.append('silence')\n",
    "\n",
    "unknownLabels = list(data['Train'].keys())\n",
    "for key in usedLabels:\n",
    "    try:\n",
    "        unknownLabels.remove(key)\n",
    "    except:\n",
    "        print(key, ' not in used')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#divding between train and test with also scaling data\n",
    "functions.train_test_creator(\n",
    "    data,\n",
    "    usedLabels,\n",
    "    unknownLabels,\n",
    "    with_unknown = False,\n",
    "    scalerType = 'robust',\n",
    "    depth = (len(data['Train'][words[0]].shape)-3)*2 + 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
