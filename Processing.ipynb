{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook\n",
    "%matplotlib notebook\n",
    "## save variables\n",
    "import pickle\n",
    "## folder names\n",
    "from glob import glob\n",
    "## standard libraries\n",
    "import numpy as np\n",
    "\n",
    "#!{sys.executable} -m pip install tensorflow-gpu --user\n",
    "#!{sys.executable} -m pip install keras --user\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "import keras\n",
    "from keras.optimizers import SGD, Adam\n",
    "#!{sys.executable} -m pip install hyperas --user\n",
    "#!{sys.executable} -m pip install networkx==1.11 --user\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "\n",
    "import os.path\n",
    "import datetime\n",
    "\n",
    "from dependencies import models\n",
    "from dependencies import functions\n",
    "\n",
    "from dependencies.convnet_drawer.convnet_drawer import Model\n",
    "from dependencies.convnet_drawer.convnet_drawer import Conv2D\n",
    "from dependencies.convnet_drawer.convnet_drawer import MaxPooling2D as MaxPooling2D_drawer\n",
    "from dependencies.convnet_drawer.convnet_drawer import Flatten as Flatten_drawer\n",
    "from dependencies.convnet_drawer.convnet_drawer import Dense as Dense_drawer\n",
    "from dependencies.convnet_drawer.matplotlib_util import save_model_to_file\n",
    "from dependencies.convnet_drawer.keras_util import convert_drawer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, labels = functions.load_dataset()\n",
    "with open('variables/labelList.pkl', 'rb') as f: \n",
    "        labelList = pickle.load(f)\n",
    "x_train = dataset['Train']\n",
    "y_train = labels['Train']\n",
    "x_test = dataset['Test']\n",
    "y_test = labels['Test']\n",
    "x_valid = dataset['Validation']\n",
    "y_valid = labels['Validation']\n",
    "x_aug = dataset['AugmentedTrain']\n",
    "y_aug = labels['AugmentedTrain']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total 0.9 of the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "\n",
    "epoch = 15\n",
    "epochSGD = 30\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.001/epoch, amsgrad=True)\n",
    "sgd = SGD(lr=0.01, decay=0.01/epochSGD, momentum=0.9, nesterov=True)\n",
    "\n",
    "top3_acc = partial(keras.metrics.top_k_categorical_accuracy, k=3)\n",
    "top3_acc.name = 'top3_acc'\n",
    "\n",
    "dest_directory = 'model_backup/'\n",
    "if not os.path.exists(dest_directory):\n",
    "      os.makedirs(dest_directory)\n",
    "\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"MODEL1\")\n",
    "cnn = models.model1(x_train,y_train, depth = x_train.shape[3])\n",
    "\n",
    "cnn.summary()\n",
    "\n",
    "\n",
    "validation_data=({'input_input': x_valid}, {'output': y_valid})\n",
    "inputData = x_train\n",
    "inputLabel = y_train\n",
    "testData = x_test\n",
    "testLabel = y_test\n",
    "validData = x_valid\n",
    "validLabel = y_valid\n",
    "loss_weights={'output': 1.}\n",
    "\n",
    "compiledAdam = cnn.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=['accuracy', top3_acc ], loss_weights=loss_weights)\n",
    "fittedAdam = cnn.fit(inputData, inputLabel,\n",
    "                     epochs=epoch,\n",
    "                     batch_size=round(x_train.shape[0]/400),\n",
    "                     shuffle=True,\n",
    "                     validation_data=validation_data, \n",
    "                     callbacks = [tbCallBack])\n",
    "\n",
    "compiledSGD = cnn.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy', top3_acc], loss_weights=loss_weights)\n",
    "fittedSGD = cnn.fit(inputData, inputLabel,\n",
    "                     epochs=epochSGD,\n",
    "                     batch_size=round(x_train.shape[0]/400),\n",
    "                     validation_data=validation_data,\n",
    "                     shuffle=True,\n",
    "                     callbacks = [tbCallBack])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = range(epoch)\n",
    "for key in fittedAdam.history:\n",
    "    ax.plot(x,fittedAdam.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = range(epochSGD)\n",
    "for key in fittedSGD.history:\n",
    "    ax.plot(x,fittedSGD.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "name = 'cnn1.bak'\n",
    "#cnn.save(dest_directory + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"\\nMODEL3\")\n",
    "cnn = models.model3(x_train,y_train, depth = x_train.shape[3], baseDim = 32)\n",
    "\n",
    "cnn.summary()\n",
    "\n",
    "\n",
    "validation_data=({'input_input': x_valid}, {'output': y_valid})\n",
    "inputData = x_train\n",
    "inputLabel = y_train\n",
    "testData = x_test\n",
    "testLabel = y_test\n",
    "validData = x_valid\n",
    "validLabel = y_valid\n",
    "loss_weights={'output': 1.}\n",
    "\n",
    "compiledAdam = cnn.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=['accuracy', top3_acc ], loss_weights=loss_weights)\n",
    "fittedAdam = cnn.fit(inputData, inputLabel,\n",
    "                     epochs=epoch,\n",
    "                     batch_size=round(x_train.shape[0]/400),\n",
    "                     shuffle=True,\n",
    "                     validation_data=validation_data, \n",
    "                     callbacks = [tbCallBack])\n",
    "\n",
    "compiledSGD = cnn.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy', top3_acc], loss_weights=loss_weights)\n",
    "fittedSGD = cnn.fit(inputData, inputLabel,\n",
    "                     epochs=epochSGD,\n",
    "                     batch_size=round(x_train.shape[0]/400),\n",
    "                     validation_data=validation_data,\n",
    "                     shuffle=True,\n",
    "                     callbacks = [tbCallBack])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = range(epoch)\n",
    "for key in fittedAdam.history:\n",
    "    ax.plot(x,fittedAdam.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = range(epochSGD)\n",
    "for key in fittedSGD.history:\n",
    "    ax.plot(x,fittedSGD.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "name = 'cnn3.bak'\n",
    "#cnn.save(dest_directory + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"\\nTINYDARKNET\")\n",
    "cnn = models.tinyDarknet(x_train,y_train, depth = x_train.shape[3], dropout = 0.05)\n",
    "\n",
    "validation_data=({'input_input': x_valid}, {'output': y_valid})\n",
    "inputData = x_train\n",
    "inputLabel = y_train\n",
    "testData = x_test\n",
    "testLabel = y_test\n",
    "validData = x_valid\n",
    "validLabel = y_valid\n",
    "loss_weights={'output': 1.}\n",
    "\n",
    "compiledAdam = cnn.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=['accuracy', top3_acc ], loss_weights=loss_weights)\n",
    "fittedAdam = cnn.fit(inputData, inputLabel,\n",
    "                     epochs=epoch,\n",
    "                     batch_size=round(x_train.shape[0]/400),\n",
    "                     shuffle=True,\n",
    "                     validation_data=validation_data, \n",
    "                     callbacks = [tbCallBack])\n",
    "\n",
    "compiledSGD = cnn.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy', top3_acc], loss_weights=loss_weights)\n",
    "fittedSGD = cnn.fit(inputData, inputLabel,\n",
    "                     epochs=epochSGD,\n",
    "                     batch_size=round(x_train.shape[0]/400),\n",
    "                     validation_data=validation_data,\n",
    "                     shuffle=True,\n",
    "                     callbacks = [tbCallBack])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = range(epoch)\n",
    "for key in fittedAdam.history:\n",
    "    ax.plot(x,fittedAdam.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = range(epochSGD)\n",
    "for key in fittedSGD.history:\n",
    "    ax.plot(x,fittedSGD.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "name = 'cnn4.bak'\n",
    "#cnn.save(dest_directory + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tensorboard --logdir Graph/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers import Flatten, Dense, Activation, BatchNormalization, Dropout, SpatialDropout2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def cBN(inputLayer, filt = 64, size = (1,1), padding = 'same', activation = 'relu', regu = 0.0, dropout = 0.05, strides = (1,1)):\n",
    "    cbn = Conv2D(filt, size, padding=padding, use_bias=False, kernel_regularizer=regularizers.l2(regu), strides = (1,1))(inputLayer)\n",
    "    cbn = Activation(activation)(cbn)\n",
    "    #cbn = BatchNormalization(epsilon=1e-05, momentum=0.1, axis=-1)(cbn)\n",
    "    return cbn\n",
    "\n",
    "def inception(inputLayer, filt = 64):\n",
    "    tower_1 = cBN(inputLayer, filt = filt)   \n",
    "    tower_1 = cBN(tower_1, size = (3,3), filt = filt)\n",
    "\n",
    "    tower_2 = cBN(inputLayer, filt = filt)\n",
    "    tower_2 = cBN(tower_2, size = (5,5), filt = filt)\n",
    "\n",
    "    tower_3 = MaxPooling2D((3,2), strides=(1,1), padding='same')(inputLayer)\n",
    "    tower_3 = cBN(tower_3, filt = filt)\n",
    "    \n",
    "    #tower_4 = cBN(inputLayer)\n",
    "\n",
    "    output = keras.layers.concatenate([tower_1, tower_2, tower_3], axis = 3)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_img = Input(name = 'input', shape = (x_train.shape[1], x_train.shape[2], x_train.shape[3]))\n",
    "\n",
    "#output = inception(input_img, filt = 20)\n",
    "output = inception(input_img, filt = 30)\n",
    "\n",
    "output = MaxPooling2D(pool_size=(3,2), padding='same')(output)\n",
    "output = SpatialDropout2D(0.15)(output)\n",
    "\n",
    "#output = inception(output,filt = 30)\n",
    "output = inception(output, filt = 40)\n",
    "\n",
    "output = MaxPooling2D(pool_size=(3,2), padding='same')(output)\n",
    "output = SpatialDropout2D(0.15)(output)\n",
    "\n",
    "#output = inception(output, filt = 40)\n",
    "output = inception(output, filt = 60)\n",
    "\n",
    "output = MaxPooling2D(pool_size=(3,2), padding='same')(output)\n",
    "output = SpatialDropout2D(0.15)(output)\n",
    "\n",
    "#output = inception(output, filt = 60)\n",
    "output = inception(output, filt = 100)\n",
    "\n",
    "output = GlobalAveragePooling2D()(output)\n",
    "#output = Dropout(0.2)(output)\n",
    "\n",
    "output = Dense(100)(output)\n",
    "#output = BatchNormalization(epsilon=1e-05, momentum=0.1)(output)\n",
    "output = Activation('relu')(output)\n",
    "output = Dropout(0.4)(output)\n",
    "output = Dense(y_train.shape[1], name = 'output', activation='softmax')(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn = Model(inputs = input_img, outputs = output)\n",
    "\n",
    "validation_data=({'input': x_valid}, {'output': y_valid})\n",
    "inputData = x_train\n",
    "inputLabel = y_train\n",
    "testData = x_test\n",
    "testLabel = y_test\n",
    "validData = x_valid\n",
    "validLabel = y_valid\n",
    "loss_weights={'output': 1.}\n",
    "\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compiledAdam = cnn.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=['accuracy', top3_acc ], loss_weights=loss_weights)\n",
    "fittedAdam = cnn.fit(inputData, inputLabel,\n",
    "                     epochs=epoch,\n",
    "                     batch_size=round(x_train.shape[0]/400),\n",
    "                     shuffle=True,\n",
    "                     validation_data=validation_data, \n",
    "                     callbacks = [tbCallBack])\n",
    "\n",
    "compiledSGD = cnn.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy', top3_acc], loss_weights=loss_weights)\n",
    "fittedSGD = cnn.fit(inputData, inputLabel,\n",
    "                     epochs=epochSGD,\n",
    "                     batch_size=round(x_train.shape[0]/400),\n",
    "                     validation_data=validation_data,\n",
    "                     shuffle=True,\n",
    "                     callbacks = [tbCallBack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = True\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "def lin_inception(inputLayer, filt = 64, base = 3):\n",
    "    tower_1 = cBN(inputLayer, filt = filt)   \n",
    "    tower_1 = cBN(tower_1, size = (base,1), filt = filt)\n",
    "\n",
    "    tower_2 = cBN(inputLayer, filt = filt)\n",
    "    tower_2 = cBN(tower_2, size = (base+1,1), filt = filt)\n",
    "\n",
    "    tower_3 = cBN(inputLayer, filt = filt)\n",
    "    tower_3 = cBN(tower_3, filt = filt, size = (base+2,1))\n",
    "    \n",
    "    output = keras.layers.concatenate([tower_1, tower_2, tower_3], axis = 3)\n",
    "    return output\n",
    "\n",
    "def inception(inputLayer, filt = 64, base = 3):\n",
    "    tower_1 = cBN(inputLayer, filt = filt)   \n",
    "    tower_1 = cBN(tower_1, size = (base,base), filt = filt)\n",
    "\n",
    "    tower_2 = cBN(inputLayer, filt = filt)\n",
    "    tower_2 = cBN(tower_2, size = (base+2,base+2), filt = filt)\n",
    "    \n",
    "    tower_3 = MaxPooling2D((3,3), strides=(1,1), padding='same')(inputLayer)\n",
    "    tower_3 = cBN(tower_3, filt = filt)\n",
    "    output = keras.layers.concatenate([tower_1, tower_2, tower_3], axis = 3)\n",
    "    return output\n",
    "\n",
    "def singleInputMFCC(x_train, name, mfcc = True):\n",
    "    single_input = Input(name = name, shape = (x_train.shape[1], x_train.shape[2], 1))\n",
    "    if mfcc:\n",
    "        output = Conv2D(120, (3,round(x_train.shape[2])))(single_input)\n",
    "        output = lin_inception(output, filt = 80, base = 3)\n",
    "        '''\n",
    "    else:\n",
    "        output = Conv2D(120, (6,6))(single_input)\n",
    "        output = inception(output, filt = 80, base = 3)\n",
    "    '''\n",
    "    else:\n",
    "        output = Conv2D(32, (3,3), padding = \"same\")(single_input)\n",
    "        output = LeakyReLU(alpha=.1)(output)\n",
    "        output = Conv2D(16, (3,3), padding = \"same\")(output)\n",
    "        output = LeakyReLU(alpha=.1)(output)\n",
    "        output = MaxPooling2D(pool_size=(2,2))(output)\n",
    "        output = Conv2D(16, (1,1), padding = \"same\")(output)\n",
    "        output = LeakyReLU(alpha=.1)(output)\n",
    "        output = Conv2D(64, (3,3), padding = \"same\")(output)\n",
    "        output = LeakyReLU(alpha=.1)(output)\n",
    "        output = Conv2D(16, (1,1), padding = \"same\")(output)\n",
    "        output = LeakyReLU(alpha=.1)(output)\n",
    "        output = Conv2D(64, (3,3), padding = \"same\")(output)\n",
    "        output = LeakyReLU(alpha=.1)(output)\n",
    "        output = MaxPooling2D(pool_size=(2,2))(output)\n",
    "        output = Conv2D(16, (1,1), padding = \"same\")(output)\n",
    "        output = LeakyReLU(alpha=.1)(output)\n",
    "        output = Conv2D(64, (3,3), padding = \"same\")(output)\n",
    "        output = LeakyReLU(alpha=.1)(output)\n",
    "        output = Conv2D(16, (1,1), padding = \"same\")(output)\n",
    "        output = LeakyReLU(alpha=.1)(output)\n",
    "        output = Conv2D(64, (3,3), padding = \"same\")(output)\n",
    "        output = LeakyReLU(alpha=.1)(output)\n",
    "        #output = MaxPooling2D(pool_size=(5,3))(output)\n",
    "        #output = inception(output, filt = 40, base = 3)       \n",
    "    \n",
    "    return single_input, output\n",
    "def firstConcat(x_train, mfcc = True):\n",
    "    first, first_output = singleInputMFCC(x_train,'first', mfcc = mfcc)\n",
    "    second, second_output = singleInputMFCC(x_train,'second', mfcc = mfcc)\n",
    "    third, third_output = singleInputMFCC(x_train,'third', mfcc = mfcc)\n",
    "    concat = keras.layers.concatenate([first_output, second_output, third_output], axis = 3)\n",
    "    return first, second, third, concat\n",
    "\n",
    "first_input, second_input, third_input, concat = firstConcat(x_train, mfcc = mfcc)\n",
    "\n",
    "if mfcc:\n",
    "    output = MaxPooling2D(pool_size=(3,1), padding='same')(concat)\n",
    "    output = Dropout(0.10)(output)\n",
    "    output = lin_inception(output, filt = 80, base = 3)\n",
    "else:\n",
    "    output = MaxPooling2D(pool_size=(5,3), padding='same')(concat)\n",
    "    #output = Dropout(0.15)(output)\n",
    "    output = inception(output, filt = 60, base = 2)\n",
    "    #output = MaxPooling2D(pool_size=(3,3), padding='same')(output)\n",
    "#output = lin_inception(output, filt = 80, base = 3)\n",
    "if mfcc:\n",
    "    output = AveragePooling2D((11,1))(output)\n",
    "    output = Flatten()(output)\n",
    "else:\n",
    "    output = GlobalAveragePooling2D()(output)\n",
    "#output = cBN(output, filt = 64)\n",
    "\n",
    "output = Dense(70)(output)\n",
    "output = Activation('relu')(output)\n",
    "output = Dropout(0.4)(output)\n",
    "\n",
    "output = Dense(y_train.shape[1], name = 'output', activation='softmax')(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn = Model(inputs = [first_input, second_input, third_input], outputs = output)\n",
    "\n",
    "validation_data=({'first': x_valid[:,:,:,0, np.newaxis], 'second': x_valid[:,:,:,1, np.newaxis], 'third': x_valid[:,:,:,2, np.newaxis]}, {'output': y_valid})\n",
    "inputData = [x_train[:,:,:,0, np.newaxis], x_train[:,:,:,1, np.newaxis], x_train[:,:,:,2, np.newaxis]]\n",
    "inputLabel = y_train\n",
    "\n",
    "testData = [x_test[:,:,:,0, np.newaxis], x_test[:,:,:,1, np.newaxis], x_test[:,:,:,2, np.newaxis]]\n",
    "testLabel = y_test\n",
    "\n",
    "validData = [x_valid[:,:,:,0, np.newaxis], x_valid[:,:,:,1, np.newaxis], x_valid[:,:,:,2, np.newaxis]]\n",
    "validLabel = y_valid\n",
    "\n",
    "loss_weights={'output': 1.}\n",
    "\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compiledAdam = cnn.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=['accuracy', top3_acc ], loss_weights=loss_weights)\n",
    "fittedAdam = cnn.fit(inputData, inputLabel,\n",
    "                     epochs=epoch,\n",
    "                     batch_size=round(x_train.shape[0]/400),\n",
    "                     shuffle=True,\n",
    "                     validation_data=validation_data, \n",
    "                     callbacks = [tbCallBack])\n",
    "\n",
    "compiledSGD = cnn.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy', top3_acc], loss_weights=loss_weights)\n",
    "fittedSGD = cnn.fit(inputData, inputLabel,\n",
    "                     epochs=epochSGD,\n",
    "                     batch_size=round(x_train.shape[0]/400),\n",
    "                     validation_data=validation_data,\n",
    "                     shuffle=True,\n",
    "                     callbacks = [tbCallBack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraClassifier(name, inputLayer, outputShape):\n",
    "    output0 = GlobalAveragePooling2D()(inputLayer)\n",
    "    output0 = Dense(100, activation = 'relu')(output0)\n",
    "    output0 = Dropout(0.7)(output0)\n",
    "    output0 = Dense( outputShape, name = name, activation='softmax')(output0)\n",
    "    return output0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(name = 'input', shape = (x_train.shape[1], x_train.shape[2], x_train.shape[3]))\n",
    "\n",
    "output = cBN(input_img)\n",
    "output = cBN(output, size = (3,3))\n",
    "output = cBN(output)\n",
    "output = cBN(output, size = (3,3))\n",
    "\n",
    "output = MaxPooling2D(pool_size=(3,2), padding='same')(output)\n",
    "\n",
    "output = inception(output)\n",
    "output0 = extraClassifier('output0', output, y_train.shape[1])\n",
    "#output = inception(output)\n",
    "\n",
    "output = MaxPooling2D(pool_size=(3,2), padding='same')(output)\n",
    "\n",
    "output = inception(output)\n",
    "output1 = extraClassifier('output1',output, y_train.shape[1])\n",
    "output = inception(output)\n",
    "\n",
    "#output = cBN(output)\n",
    "output = GlobalAveragePooling2D()(output)\n",
    "\n",
    "#output = Dense(90, activation = 'relu')(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output2 = Dense(y_train.shape[1], name = 'output2', activation='softmax')(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Model(inputs = input_img, outputs = [output2, output1, output0])\n",
    "\n",
    "inputData = x_train\n",
    "inputLabel = [y_train, y_train, y_train]\n",
    "validation_data=({'input': x_valid}, {'output0': y_valid, 'output1': y_valid, 'output2': y_valid})\n",
    "testData = x_test\n",
    "testLabel = [y_test, y_test, y_test]\n",
    "validData = x_valid\n",
    "validLabel = [y_valid, y_valid, y_valid]\n",
    "loss_weights={'output0': 1., 'output1': 1., 'output2' : 1.}\n",
    "\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compiledAdam = cnn.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=['accuracy', top3_acc ], loss_weights=loss_weights)\n",
    "fittedAdam = cnn.fit(inputData, inputLabel,\n",
    "                     epochs=epoch,\n",
    "                     batch_size=round(x_train.shape[0]/400),\n",
    "                     shuffle=True,\n",
    "                     validation_data=validation_data, \n",
    "                     callbacks = [tbCallBack])\n",
    "\n",
    "loss_weights={'output0': 0.4, 'output1': 0.7, 'output2' : 1.}\n",
    "compiledSGD = cnn.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy', top3_acc], loss_weights=loss_weights)\n",
    "fittedSGD = cnn.fit(inputData, inputLabel,\n",
    "                     epochs=epochSGD,\n",
    "                     batch_size=round(x_train.shape[0]/400),\n",
    "                     validation_data=validation_data,\n",
    "                     shuffle=True,\n",
    "                     callbacks = [tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dest_directory = 'model_backup/'\n",
    "name = 'cnn.bak'\n",
    "\n",
    "cnn = load_model(dest_directory + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def w_categorical_crossentropy(y_true, y_pred, weights):\n",
    "    nb_cl = len(weights)\n",
    "    final_mask = k.zeros_like(y_pred[:, 0])\n",
    "    y_pred_max = k.max(y_pred, axis=1)\n",
    "    y_pred_max = k.reshape(y_pred_max, (k.shape(y_pred)[0], 1))\n",
    "    y_pred_max_mat = k.cast(k.equal(y_pred, y_pred_max), k.floatx())\n",
    "    for c_p, c_t in product(range(nb_cl), range(nb_cl)):\n",
    "        final_mask += (weights[c_t, c_p] * y_pred_max_mat[:, c_p] * y_true[:, c_t])\n",
    "    return k.categorical_crossentropy(y_pred, y_true) * final_mask\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = k.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= k.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = k.clip(y_pred, k.epsilon(), 1 - k.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * k.log(y_pred) * weights\n",
    "        loss = -k.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#find prediction with test data\n",
    "%matplotlib notebook\n",
    "preds = cnn.predict([x_test[:,:,:,0, np.newaxis], x_test[:,:,:,1, np.newaxis], x_test[:,:,:,2, np.newaxis]])\n",
    "#Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "functions.plot_confusion_matrix(preds, y_test, classes=labelList, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "precision = cnn.evaluate([x_test[:,:,:,0, np.newaxis], x_test[:,:,:,1, np.newaxis], x_test[:,:,:,2, np.newaxis]],  y_test)\n",
    "print (\"Precision: \", round(precision[1]*100,2),\"%\")\n",
    "\n",
    "#print(list(used))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cnn.predict(validData)\n",
    "plt.figure()\n",
    "cm = functions.plot_confusion_matrix(preds, validLabel, classes=labelList, normalize=True,\n",
    "                      title='Normalized confusion matrix', plot = True)\n",
    "weights = (1 + cm)\n",
    "weights /= np.max(weights)\n",
    "print(weights**(-1))\n",
    "ncce = partial(w_categorical_crossentropy, weights=weights)\n",
    "ncce.__name__ ='w_categorical_crossentropy'\n",
    "\n",
    "weights = np.diagonal(1 - cm).copy()\n",
    "#weights /= np.max(weights)\n",
    "ncce1 = weighted_categorical_crossentropy(weights)\n",
    "ncce1.__name__ ='w_categorical_crossentropy'\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiledSGD = cnn.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy', top3_acc], loss_weights=loss_weights)\n",
    "fittedSGD = cnn.fit(inputData, inputLabel,\n",
    "                     epochs=epochSGD,\n",
    "                     batch_size=round(x_train.shape[0]/400),\n",
    "                     validation_data=validation_data,\n",
    "                     shuffle=True,\n",
    "                     callbacks = [tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cnn.predict(x_test)\n",
    "#Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "functions.plot_confusion_matrix(preds, y_test, classes=labelList, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "precision = cnn.evaluate(x_test,  y_test)\n",
    "#print (\"Precision: \", round(precision*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output of conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_copy = Sequential()\n",
    "cnn_copy.add(cnn.layers[0])\n",
    "result = cnn_copy.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(result[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_directory = 'model_backup/'\n",
    "if not os.path.exists(dest_directory):\n",
    "      os.makedirs(dest_directory)\n",
    "name = 'cnn.bak'\n",
    "cnn.save(dest_directory + name)\n",
    "\n",
    "#bak = load_model(dest_directory + name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_shape=(x_train.shape[1], x_train.shape[2],1))\n",
    "model.add(Conv2D(100, (4,4),  strides = (1,1), padding=\"valid\"))\n",
    "model.add(Conv2D(100, (4,2),  strides = (1,1), padding=\"valid\"))\n",
    "model.add(MaxPooling2D_drawer(pool_size=(3,3)))\n",
    "model.add(Conv2D(128, (4,2),  strides = (1,1), padding=\"valid\"))\n",
    "model.add(Conv2D(128, (5,2),  strides = (1,1), padding=\"valid\"))\n",
    "model.add(MaxPooling2D_drawer(pool_size=(4,1)))\n",
    "model.add(Flatten_drawer())\n",
    "model.add(Dense_drawer(100))\n",
    "model.add(Dense_drawer(y_train.shape[1]))\n",
    "\n",
    "#save to pdf\n",
    "save_model_to_file(model, \"example.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking for hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.activations import softmax\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, Convolution2D, MaxPooling2D, AveragePooling2D, BatchNormalization\n",
    "trials = Trials()\n",
    "best_run, best_model = optim.minimize(model=functions.create_model, \n",
    "                                      data=functions.data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=100,\n",
    "                                      trials=trials,\n",
    "                                      notebook_name='Processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_directory = 'model_backup/'\n",
    "'''\n",
    "best_model = load_model(dest_directory + 'best_model.bak')\n",
    "\n",
    "with open(dest_directory+'best_run.pkl', 'rb') as f:  \n",
    "    best_run = pickle.load(f)    \n",
    "'''\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(x_test, y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)\n",
    "best_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "dest_directory_temp =dest_directory + 'bestModel('+now.strftime(\"%m-%d %H.%M\")+\")\"\n",
    "if not os.path.exists(dest_directory_temp):\n",
    "      os.makedirs(dest_directory_temp)\n",
    "best_model.save(dest_directory_temp + '/best_model.bak')\n",
    "\n",
    "with open(dest_directory_temp + '/best_run.pkl', 'wb') as f:  \n",
    "    pickle.dump(best_run, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.best_trial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
