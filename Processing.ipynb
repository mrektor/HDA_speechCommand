{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook\n",
    "%matplotlib notebook\n",
    "## save variables\n",
    "import pickle\n",
    "## folder names\n",
    "from glob import glob\n",
    "## standard libraries\n",
    "import numpy as np\n",
    "\n",
    "##\n",
    "#from sklearn.decomposition import PCA\n",
    "#from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "#!{sys.executable} -m pip install tensorflow-gpu --user\n",
    "#!{sys.executable} -m pip install keras --user\n",
    "import keras\n",
    "\n",
    "#!{sys.executable} -m pip install hyperas --user\n",
    "#!{sys.executable} -m pip install networkx==1.11 --user\n",
    "#!{sys.executable} -m pip install jupyter-tensorboard --user\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "\n",
    "import os.path\n",
    "import datetime\n",
    "\n",
    "from dependencies import models\n",
    "from dependencies import functions\n",
    "\n",
    "from dependencies.convnet_drawer.convnet_drawer import Model\n",
    "from dependencies.convnet_drawer.convnet_drawer import Conv2D\n",
    "from dependencies.convnet_drawer.convnet_drawer import MaxPooling2D as MaxPooling2D_drawer\n",
    "from dependencies.convnet_drawer.convnet_drawer import Flatten as Flatten_drawer\n",
    "from dependencies.convnet_drawer.convnet_drawer import Dense as Dense_drawer\n",
    "from dependencies.convnet_drawer.matplotlib_util import save_model_to_file\n",
    "from dependencies.convnet_drawer.keras_util import convert_drawer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import mfcc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded variables/mfccDict[nC=15 wL=0.025 wS=0.01].pkl\n"
     ]
    }
   ],
   "source": [
    "#choose which dictionary to use\n",
    "choice = 'mfcc'#'spectro'\n",
    "\n",
    "\n",
    "selected = 1\n",
    "if choice == 'mfcc':\n",
    "    \n",
    "    #retrieving of used values for the computation of mfcc\n",
    "    with open('variables/mfccValues.pkl', 'rb') as f:  \n",
    "        values = pickle.load(f)\n",
    "    \n",
    "    #name format of the selected data\n",
    "    name = 'variables/mfccDict[nC='+str(values[selected][0])+' wL='+str(values[selected][2])+' wS='+str(values[selected][3])+'].pkl'\n",
    "    \n",
    "    #saving in usedDict of the mfcc dict\n",
    "    with open(name, 'rb') as f: \n",
    "        usedDict = pickle.load(f)\n",
    "    print('Loaded '+name)\n",
    "\n",
    "elif choice == 'spectro':\n",
    "    \n",
    "    #saving in usedDict of the spectro dict\n",
    "    with open('variables/spectroDict.pkl', 'rb') as f:  \n",
    "        usedDict = pickle.load(f)\n",
    "    print('Loaded spectroram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring and scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#core words of the dataset\n",
    "coreKey = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", \"zero\",\n",
    "           \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
    "\n",
    "#split of the core set\n",
    "numbers = ['one', 'two', 'three','four','five','six','seven','eight','nine', \"zero\"]\n",
    "words = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\"]\n",
    "\n",
    "test = [\"yes\", \"up\", \"down\", \"left\"]#, \"right\", \"on\", \"off\", \"stop\", \"go\", \"zero\",\n",
    "          # \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
    "\n",
    "#selecting the subset of words\n",
    "used = words\n",
    "used.append('silence')\n",
    "unknown = list(usedDict.keys())\n",
    "for key in used:\n",
    "    try:\n",
    "        unknown.remove(key)\n",
    "    except:\n",
    "        print(key, ' not in used')\n",
    "\n",
    "#divding between train and test with also scaling data\n",
    "x_train, y_train, x_test, y_test, labelList = functions.train_test_creator(\n",
    "    {k: usedDict[k] for k in usedDict.keys() & used },\n",
    "    {k: usedDict[k] for k in usedDict.keys() & unknown },\n",
    "    with_unknown = True,\n",
    "    scalerType = 'robust')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "epoch = 40\n",
    "\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "cnn = models.tinyDarknet(x_train,y_train)\n",
    "\n",
    "cnn.summary()\n",
    "\n",
    "\n",
    "compiledAdam = cnn.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "fittedAdam = cnn.fit(x_train, y_train,\n",
    "                     epochs=epoch,\n",
    "                     validation_data=(x_test, y_test),\n",
    "                     batch_size=round(x_train.shape[0]/250),\n",
    "                     shuffle=True,\n",
    "                     callbacks = [tbCallBack])\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=0.0005, momentum=0.9, nesterov=True)\n",
    "compiledSGD = cnn.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "fittedSGD = cnn.fit(x_train, y_train, \n",
    "                epochs=epoch, \n",
    "                validation_data=(x_test, y_test), \n",
    "                batch_size=round(x_train.shape[0]/100), \n",
    "                shuffle=True,\n",
    "                callbacks = [tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tensorboard --logdir Graph/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dest_directory = 'model_backup/'\n",
    "name = 'cnn.bak'\n",
    "\n",
    "cnn = load_model(dest_directory + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#find prediction with test data\n",
    "preds = cnn.predict(x_test)\n",
    "print(list(used))\n",
    "\n",
    "#plot confusion matrix\n",
    "plotConfusionMatrix(preds,y_test,list(used))\n",
    "loss, precision = cnn.evaluate(x_test,y_test)\n",
    "print (\"Precision: \", round(precision*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = range(epoch)\n",
    "for key in fittedAdam.history:\n",
    "    ax.plot(x,fittedAdam.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output of conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_copy = Sequential()\n",
    "cnn_copy.add(cnn.layers[0])\n",
    "result = cnn_copy.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(result[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_directory = 'model_backup/'\n",
    "if not os.path.exists(dest_directory):\n",
    "      os.makedirs(dest_directory)\n",
    "name = 'cnn.bak'\n",
    "cnn.save(dest_directory + name)\n",
    "\n",
    "#bak = load_model(dest_directory + name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_shape=(x_train.shape[1], x_train.shape[2],1))\n",
    "model.add(Conv2D(100, (4,4),  strides = (1,1), padding=\"valid\"))\n",
    "model.add(Conv2D(100, (4,2),  strides = (1,1), padding=\"valid\"))\n",
    "model.add(MaxPooling2D_drawer(pool_size=(3,3)))\n",
    "model.add(Conv2D(128, (4,2),  strides = (1,1), padding=\"valid\"))\n",
    "model.add(Conv2D(128, (5,2),  strides = (1,1), padding=\"valid\"))\n",
    "model.add(MaxPooling2D_drawer(pool_size=(4,1)))\n",
    "model.add(Flatten_drawer())\n",
    "model.add(Dense_drawer(100))\n",
    "model.add(Dense_drawer(y_train.shape[1]))\n",
    "\n",
    "#save to pdf\n",
    "save_model_to_file(model, \"example.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking for hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data():\n",
    "    #load used data\n",
    "    with open('variables/train_test_split.pkl', 'rb') as f: \n",
    "        x_train = pickle.load(f)\n",
    "        y_train = pickle.load(f)\n",
    "        x_test = pickle.load(f)\n",
    "        y_test = pickle.load(f) \n",
    "    return x_train, y_train, x_test, y_test \n",
    "\n",
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    activation = 'softplus'\n",
    "    minim = {{choice([8,16,20,24,32,30,46,50,64])}}\n",
    "    padding = 'same'\n",
    "    cnn = Sequential()\n",
    "\n",
    "    cnn.add(Convolution2D(minim, (4,2),  strides = (1,1), padding=\"valid\", \n",
    "                          input_shape=(x_train.shape[1], x_train.shape[2],1)))\n",
    "    cnn.add(Activation(activation))\n",
    "\n",
    "    cnn.add(Convolution2D(minim * 2, (2,2),  strides = (1,1), padding=padding))\n",
    "    cnn.add(Activation(activation))\n",
    "\n",
    "\n",
    "    cnn.add(Convolution2D(minim*4, (2,2),  strides = (1,1), padding=padding))\n",
    "    cnn.add(Activation(activation))\n",
    "\n",
    "    cnn.add(BatchNormalization())\n",
    "\n",
    "    cnn.add(MaxPooling2D(pool_size=(4,2)))\n",
    "\n",
    "    cnn.add(Dropout(0.4))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(Convolution2D(minim * 2, (2,2),  strides = (1,1), padding=padding ))\n",
    "    cnn.add(Activation(activation))\n",
    "\n",
    "    #cnn.add(Dropout(0.2))\n",
    "    cnn.add(Convolution2D(minim * 4, (2,2),  strides = (1,1), padding=padding))\n",
    "    cnn.add(Activation(activation))\n",
    "    cnn.add(BatchNormalization())\n",
    "\n",
    "    cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    cnn.add(Dropout(0.4))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(Convolution2D(minim *8, (2,2),  strides = (1,1), padding=padding))\n",
    "    cnn.add(Activation(activation))\n",
    "\n",
    "\n",
    "\n",
    "    cnn.add(MaxPooling2D(pool_size=(4,2)))\n",
    "\n",
    "    cnn.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "    cnn.add(Flatten())\n",
    "\n",
    "    cnn.add(Dense(80, activation=activation))\n",
    "\n",
    "    cnn.add(Dropout(0.5))\n",
    "    cnn.add(BatchNormalization())\n",
    "\n",
    "    cnn.add(Dense(y_train.shape[1], activation=\"softplus\"))\n",
    "\n",
    "    cnn.compile(loss=\"categorical_crossentropy\", optimizer=\"adamax\", metrics=['accuracy'])\n",
    "            \n",
    "    cnn.fit(x_train, y_train,\n",
    "              batch_size={{choice([128, 256])}},\n",
    "              epochs=30,\n",
    "              verbose=2,\n",
    "              validation_data=(x_test, y_test))\n",
    "    score, acc = cnn.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': cnn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from keras.layers.core import Dense, Dropout, Activation\n",
    "trials = Trials()\n",
    "best_run, best_model = optim.minimize(model=create_model, \n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=100,\n",
    "                                      trials=trials,\n",
    "                                      notebook_name='Processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_directory = 'model_backup/'\n",
    "'''\n",
    "best_model = load_model(dest_directory + 'best_model.bak')\n",
    "\n",
    "with open(dest_directory+'best_run.pkl', 'rb') as f:  \n",
    "    best_run = pickle.load(f)    \n",
    "'''\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(x_test, y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)\n",
    "best_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "dest_directory_temp =dest_directory + 'bestModel('+now.strftime(\"%m-%d %H.%M\")+\")\"\n",
    "if not os.path.exists(dest_directory_temp):\n",
    "      os.makedirs(dest_directory_temp)\n",
    "best_model.save(dest_directory_temp + '/best_model.bak')\n",
    "\n",
    "with open(dest_directory_temp + '/best_run.pkl', 'wb') as f:  \n",
    "    pickle.dump(best_run, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
