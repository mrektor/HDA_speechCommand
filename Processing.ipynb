{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook\n",
    "%matplotlib notebook\n",
    "## save variables\n",
    "import pickle\n",
    "## folder names\n",
    "from glob import glob\n",
    "## standard libraries\n",
    "import numpy as np\n",
    "\n",
    "#!{sys.executable} -m pip install tensorflow-gpu --user\n",
    "#!{sys.executable} -m pip install keras --user\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "import keras\n",
    "from keras.optimizers import SGD, Adam\n",
    "#!{sys.executable} -m pip install hyperas --user\n",
    "#!{sys.executable} -m pip install networkx==1.11 --user\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "\n",
    "import os.path\n",
    "import datetime\n",
    "\n",
    "from dependencies import models\n",
    "from dependencies import functions\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['name', 'acc', 'val_acc', 'val_top3_acc', 'test_acc',\n",
    "                           'feature', 'delta', 'inference_time', 'augmented_set', 'parameters', 'epochSGD', 'epochAdam'])\n",
    "line = {'name': None, 'acc':None, 'val_acc':None, 'val_top3_acc':None, 'test_acc':None,\n",
    "                           'feature':None, 'delta':None, 'inference_time':None, \n",
    "                            'augmented_set':None, 'parameters':None, 'epochSGD': None, 'epochAdam' : None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose which dictionary to use\n",
    "choice =    'mfcc'\n",
    "useDelta =  True\n",
    "\n",
    "data = {}\n",
    "    \n",
    "selected = 0\n",
    "\n",
    "if choice == 'mfcc':\n",
    "    #retrieving of used values for the computation of mfcc\n",
    "    with open('variables/mfccValues.pkl', 'rb') as f:  \n",
    "        values = pickle.load(f)\n",
    "    for index in range(4):\n",
    "        #name format of the selected data\n",
    "        if useDelta:\n",
    "            name = 'variables/mfccDictDD'+functions.getName(index)+'[nC='+str(values[selected][0])+' wL='+str(values[selected][2])+' wS='+str(values[selected][3])+'].pkl'\n",
    "        else:\n",
    "            name = 'variables/mfccDict'+functions.getName(index)+'[nC='+str(values[selected][0])+' wL='+str(values[selected][2])+' wS='+str(values[selected][3])+'].pkl'\n",
    "        #loading in usedDict of the mfcc dict\n",
    "        with open(name, 'rb') as f: \n",
    "            data[functions.getName(index)] = pickle.load(f)\n",
    "        print('Loaded '+name)\n",
    "\n",
    "elif choice == 'logfilter':\n",
    "    #retrieving of used values for the computation of mfcc\n",
    "    with open('variables/lfValues.pkl', 'rb') as f:  \n",
    "        values = pickle.load(f)\n",
    "    for index in range(4):\n",
    "        #name format of the selected data\n",
    "        if useDelta:\n",
    "            name = 'variables/logfiltDictDD'+functions.getName(index)+'[nF='+str(values[selected][0])+' wL='+str(values[selected][1])+' wS='+str(values[selected][2])+'].pkl'\n",
    "        else:\n",
    "            name = 'variables/logfiltDict'+functions.getName(index)+'[nF='+str(values[selected][0])+' wL='+str(values[selected][1])+' wS='+str(values[selected][2])+'].pkl'\n",
    "        #saving in usedDict of the logfilter dict\n",
    "        with open(name, 'rb') as f:  \n",
    "            data[functions.getName(index)] = pickle.load(f)\n",
    "        print('Loaded '+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#core words of the dataset\n",
    "coreKey = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", \"zero\",\n",
    "           \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
    "\n",
    "#split of the core set\n",
    "numbers = ['one', 'two', 'three','four','five','six','seven','eight','nine', \"zero\"]\n",
    "\n",
    "words = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\"]\n",
    "\n",
    "#selecting the subset of words to predict\n",
    "usedLabels = words\n",
    "\n",
    "usedLabels.append('silence')\n",
    "\n",
    "unknownLabels = list(data['Train'].keys())\n",
    "for key in usedLabels:\n",
    "    try:\n",
    "        unknownLabels.remove(key)\n",
    "    except:\n",
    "        print(key, ' not in used')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#divding between train and test with also scaling data\n",
    "functions.train_test_creator(\n",
    "    data,\n",
    "    usedLabels,\n",
    "    unknownLabels,\n",
    "    with_unknown = False,\n",
    "    scalerType = 'robust',\n",
    "    depth = (len(data['Train'][words[0]].shape)-3)*2 + 1,\n",
    "    unknown_percentage = 0.3)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, labels = functions.load_dataset()\n",
    "with open('variables/labelList.pkl', 'rb') as f: \n",
    "        labelList = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total 0.8 of the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "\n",
    "epoch = 20\n",
    "epochSGD = 20\n",
    "\n",
    "epochs = [epoch, epochSGD]\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.001/epoch, amsgrad=True)\n",
    "sgd = SGD(lr=0.001, decay=0.001/epochSGD, momentum=0.9, nesterov=True)\n",
    "\n",
    "optimizers = [adam, sgd]\n",
    "\n",
    "top3_acc = partial(keras.metrics.top_k_categorical_accuracy, k=3)\n",
    "top3_acc.name = 'top3_acc'\n",
    "\n",
    "titles = ['Adam History', 'SGD History']\n",
    "\n",
    "dest_directory = 'model_backup/'\n",
    "if not os.path.exists(dest_directory):\n",
    "      os.makedirs(dest_directory)\n",
    "\n",
    "esCallBack = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=3, verbose=1, mode='auto', baseline=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "name = 'model1'\n",
    "print(name)\n",
    "table = line\n",
    "table['name'] =  name\n",
    "table['augmented_set'] =  'No'\n",
    "\n",
    "inputData, inputLabel, testData, testLabel, validData, validLabel, augmentedData, augmentedLabel, validation_data, loss_weights = functions.modelSelection(name, dataset, labels)\n",
    "\n",
    "cnn = models.model1(inputData,inputLabel)\n",
    "\n",
    "fittedHistory = []\n",
    "\n",
    "for count, optimizer in enumerate(optimizers):\n",
    "    print('Using optimizer number ' + str(count))\n",
    "    cnn.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy', top3_acc ], loss_weights=loss_weights)\n",
    "    fittedHistory.append(cnn.fit(inputData, inputLabel,\n",
    "                         epochs=epochs[count],\n",
    "                         batch_size=round(inputData.shape[0]/400),\n",
    "                         shuffle=True,\n",
    "                         validation_data=validation_data,\n",
    "                         callbacks = [esCallBack]))\n",
    "\n",
    "functions.plotHistory(epochs, fittedHistory, 'Training History')\n",
    "\n",
    "for count, fitted in enumerate(fittedHistory):\n",
    "    n_epochs = len(fitted.history['loss'])\n",
    "    if count == 0:\n",
    "        table['epochSGD'] =  n_epochs\n",
    "    else:\n",
    "        table['epochAdam'] =  n_epochs\n",
    "\n",
    "table['feature'] =  choice\n",
    "table['delta'] =  useDelta\n",
    "table['parameters'] =  cnn.count_params()\n",
    "\n",
    "for key in fittedHistory[-1].history:\n",
    "    if key in df.keys():\n",
    "        table[key] =  fittedHistory[-1].history[key][-1]\n",
    "        \n",
    "micros = int(round(time.time() * 1000000))\n",
    "# 100 times the prediction of test data\n",
    "for i in range(1):\n",
    "    preds = cnn.predict(testData)\n",
    "    \n",
    "micros = time.time() * 1000000 - micros\n",
    "# normalize the time for single prediction\n",
    "micros = micros/100/testData.shape[0]\n",
    "\n",
    "table['inference_time'] =  micros\n",
    "\n",
    "#Plot normalized confusion matrix\n",
    "functions.plot_confusion_matrix(preds, testLabel, classes=labelList, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "precision = cnn.evaluate(testData,  testLabel)\n",
    "print (\"Precision: \", round(precision[1]*100,2),\"%\")\n",
    "\n",
    "table['test_acc'] =  round(precision[1],4)\n",
    "\n",
    "\n",
    "df = df.append(table, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "table['augmented_set'] =  'Yes'\n",
    "cnn = models.model1(inputData,inputLabel)\n",
    "\n",
    "print('Adding augmented dataset')\n",
    "inputData, inputLabel = functions.meltData(inputData, augmentedData, inputLabel, augmentedLabel, 0.7)\n",
    "\n",
    "fittedHistory = []\n",
    "\n",
    "for count, optimizer in enumerate(optimizers):\n",
    "    print('Using optimizer number ' + str(count))\n",
    "    cnn.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy', top3_acc ], loss_weights=loss_weights)\n",
    "    fittedHistory.append(cnn.fit(inputData, inputLabel,\n",
    "                         epochs=3,#epochs[count],\n",
    "                         batch_size=round(inputData.shape[0]/400),\n",
    "                         shuffle=True,\n",
    "                         validation_data=validation_data,\n",
    "                         callbacks = [esCallBack]))\n",
    "\n",
    "    \n",
    "functions.plotHistory(epochs, fittedHistory, 'Training History')\n",
    "\n",
    "for count, fitted in enumerate(fittedHistory):\n",
    "    n_epochs = len(fitted.history['loss'])\n",
    "    if count == 0:\n",
    "        table['epochSGD'] =  n_epochs\n",
    "    else:\n",
    "        table['epochAdam'] =  n_epochs\n",
    "        \n",
    "table['parameters'] =  cnn.count_params()\n",
    "\n",
    "for key in fittedHistory[-1].history:\n",
    "    if key in df.keys():\n",
    "        table[key] =  fittedHistory[-1].history[key][-1]\n",
    "        \n",
    "micros = int(round(time.time() * 1000000))\n",
    "# 100 times the prediction of test data\n",
    "for i in range(100):\n",
    "    preds = cnn.predict(testData)\n",
    "    \n",
    "micros = time.time() * 1000000 - micros\n",
    "# normalize the time for single prediction\n",
    "micros = micros/100/testData.shape[0]\n",
    "\n",
    "table['inference_time'] =  micros\n",
    "\n",
    "#Plot normalized confusion matrix\n",
    "functions.plot_confusion_matrix(preds, testLabel, classes=labelList, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "precision = cnn.evaluate(testData,  testLabel)\n",
    "print (\"Precision: \", round(precision[1]*100,2),\"%\")\n",
    "\n",
    "table['test_acc'] =  round(precision[1],4)\n",
    "\n",
    "\n",
    "df = df.append(table, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "name = 'model2'\n",
    "print(name)\n",
    "table = line\n",
    "table['name'] =  name\n",
    "table['augmented_set'] =  'No'\n",
    "\n",
    "inputData, inputLabel, testData, testLabel, validData, validLabel, augmentedData, augmentedLabel, validation_data, loss_weights = functions.modelSelection(name, dataset, labels)\n",
    "\n",
    "cnn = models.model2(inputData,inputLabel, baseDim = 40)\n",
    "\n",
    "fittedHistory = []\n",
    "\n",
    "for count, optimizer in enumerate(optimizers):\n",
    "    print('Using optimizer number ' + str(count))\n",
    "    cnn.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy', top3_acc ], loss_weights=loss_weights)\n",
    "    fittedHistory.append(cnn.fit(inputData, inputLabel,\n",
    "                         epochs=epochs[count],\n",
    "                         batch_size=round(inputData.shape[0]/400),\n",
    "                         shuffle=True,\n",
    "                         validation_data=validation_data,\n",
    "                         callbacks = [esCallBack]))\n",
    "\n",
    "\n",
    "functions.plotHistory(epochs, fittedHistory, 'Training History')\n",
    "\n",
    "for count, fitted in enumerate(fittedHistory):\n",
    "    n_epochs = len(fitted.history['loss'])\n",
    "    if count == 0:\n",
    "        table['epochSGD'] =  n_epochs\n",
    "    else:\n",
    "        table['epochAdam'] =  n_epochs\n",
    "\n",
    "\n",
    "table['feature'] =  choice\n",
    "table['delta'] =  useDelta\n",
    "table['parameters'] =  cnn.count_params()\n",
    "\n",
    "for key in fittedHistory[-1].history:\n",
    "    if key in df.keys():\n",
    "        table[key] =  fittedHistory[-1].history[key][-1]\n",
    "        \n",
    "micros = int(round(time.time() * 1000000))\n",
    "# 100 times the prediction of test data\n",
    "for i in range(100):\n",
    "    preds = cnn.predict(testData)\n",
    "    \n",
    "micros = time.time() * 1000000 - micros\n",
    "# normalize the time for single prediction\n",
    "micros = micros/100/testData.shape[0]\n",
    "\n",
    "table['inference_time'] =  micros\n",
    "\n",
    "#Plot normalized confusion matrix\n",
    "functions.plot_confusion_matrix(preds, testLabel, classes=labelList, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "precision = cnn.evaluate(testData,  testLabel)\n",
    "print (\"Precision: \", round(precision[1]*100,2),\"%\")\n",
    "\n",
    "table['test_acc'] =  round(precision[1],4)\n",
    "\n",
    "\n",
    "df = df.append(table, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "table['augmented_set'] =  'Yes'\n",
    "cnn = models.model2(inputData,inputLabel, baseDim = 40 )\n",
    "\n",
    "print('Adding augmented dataset')\n",
    "inputData, inputLabel = functions.meltData(inputData, augmentedData, inputLabel, augmentedLabel, 0.7)\n",
    "\n",
    "fittedHistory = []\n",
    "\n",
    "for count, optimizer in enumerate(optimizers):\n",
    "    print('Using optimizer number ' + str(count))\n",
    "    cnn.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy', top3_acc ], loss_weights=loss_weights)\n",
    "    fittedHistory.append(cnn.fit(inputData, inputLabel,\n",
    "                         epochs=epochs[count],\n",
    "                         batch_size=round(inputData.shape[0]/400),\n",
    "                         shuffle=True,\n",
    "                         validation_data=validation_data,\n",
    "                         callbacks = [esCallBack]))\n",
    "\n",
    "functions.plotHistory(epochs, fittedHistory, 'Training History')\n",
    "\n",
    "for count, fitted in enumerate(fittedHistory):\n",
    "    n_epochs = len(fitted.history['loss'])\n",
    "    if count == 0:\n",
    "        table['epochSGD'] =  n_epochs\n",
    "    else:\n",
    "        table['epochAdam'] =  n_epochs\n",
    "        \n",
    "table['parameters'] =  cnn.count_params()\n",
    "\n",
    "for key in fittedHistory[-1].history:\n",
    "    if key in df.keys():\n",
    "        table[key] =  fittedHistory[-1].history[key][-1]\n",
    "        \n",
    "micros = int(round(time.time() * 1000000))\n",
    "# 100 times the prediction of test data\n",
    "for i in range(100):\n",
    "    preds = cnn.predict(testData)\n",
    "    \n",
    "micros = time.time() * 1000000 - micros\n",
    "# normalize the time for single prediction\n",
    "micros = micros/100/testData.shape[0]\n",
    "\n",
    "table['inference_time'] =  micros\n",
    "\n",
    "#Plot normalized confusion matrix\n",
    "functions.plot_confusion_matrix(preds, testLabel, classes=labelList, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "precision = cnn.evaluate(testData,  testLabel)\n",
    "print (\"Precision: \", round(precision[1]*100,2),\"%\")\n",
    "\n",
    "table['test_acc'] =  round(precision[1],4)\n",
    "\n",
    "\n",
    "df = df.append(table, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TinyDarknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "name = 'tinyDarknet'\n",
    "print(name)\n",
    "table = line\n",
    "table['name'] =  name\n",
    "table['augmented_set'] =  'No'\n",
    "\n",
    "inputData, inputLabel, testData, testLabel, validData, validLabel, augmentedData, augmentedLabel, validation_data, loss_weights = functions.modelSelection(name, dataset, labels)\n",
    "\n",
    "cnn = models.tinyDarknet(inputData,inputLabel, dropout = 0.3)\n",
    "\n",
    "fittedHistory = []\n",
    "\n",
    "for count, optimizer in enumerate(optimizers):\n",
    "    print('Using optimizer number ' + str(count))\n",
    "    cnn.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy', top3_acc ], loss_weights=loss_weights)\n",
    "    fittedHistory.append(cnn.fit(inputData, inputLabel,\n",
    "                         epochs=epochs[count],\n",
    "                         batch_size=round(inputData.shape[0]/400),\n",
    "                         shuffle=True,\n",
    "                         validation_data=validation_data,\n",
    "                         callbacks = [esCallBack]))\n",
    "\n",
    "functions.plotHistory(epochs, fittedHistory, 'Training History')\n",
    "\n",
    "for count, fitted in enumerate(fittedHistory):\n",
    "    n_epochs = len(fitted.history['loss'])\n",
    "    if count == 0:\n",
    "        table['epochSGD'] =  n_epochs\n",
    "    else:\n",
    "        table['epochAdam'] =  n_epochs\n",
    "        \n",
    "table['feature'] =  choice\n",
    "table['delta'] =  useDelta\n",
    "table['parameters'] =  cnn.count_params()\n",
    "\n",
    "for key in fittedHistory[-1].history:\n",
    "    if key in df.keys():\n",
    "        table[key] =  fittedHistory[-1].history[key][-1]\n",
    "        \n",
    "micros = int(round(time.time() * 1000000))\n",
    "# 100 times the prediction of test data\n",
    "for i in range(100):\n",
    "    preds = cnn.predict(testData)\n",
    "    \n",
    "micros = time.time() * 1000000 - micros\n",
    "# normalize the time for single prediction\n",
    "micros = micros/100/testData.shape[0]\n",
    "\n",
    "table['inference_time'] =  micros\n",
    "\n",
    "#Plot normalized confusion matrix\n",
    "functions.plot_confusion_matrix(preds, testLabel, classes=labelList, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "precision = cnn.evaluate(testData,  testLabel)\n",
    "print (\"Precision: \", round(precision[1]*100,2),\"%\")\n",
    "\n",
    "table['test_acc'] =  round(precision[1],4)\n",
    "\n",
    "\n",
    "df = df.append(table, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "table['augmented_set'] =  'Yes'\n",
    "cnn = models.tinyDarknet(inputData,inputLabel, dropout = 0.3)\n",
    "\n",
    "print('Adding augmented dataset')\n",
    "inputData, inputLabel = functions.meltData(inputData, augmentedData, inputLabel, augmentedLabel, 0.7)\n",
    "\n",
    "fittedHistory = []\n",
    "\n",
    "for count, optimizer in enumerate(optimizers):\n",
    "    print('Using optimizer number ' + str(count))\n",
    "    cnn.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy', top3_acc ], loss_weights=loss_weights)\n",
    "    fittedHistory.append(cnn.fit(inputData, inputLabel,\n",
    "                         epochs=epochs[count],\n",
    "                         batch_size=round(inputData.shape[0]/400),\n",
    "                         shuffle=True,\n",
    "                         validation_data=validation_data,\n",
    "                         callbacks = [esCallBack]))\n",
    "    \n",
    "functions.plotHistory(epochs, fittedHistory, 'Training History')\n",
    "\n",
    "for count, fitted in enumerate(fittedHistory):\n",
    "    n_epochs = len(fitted.history['loss'])\n",
    "    if count == 0:\n",
    "        table['epochSGD'] =  n_epochs\n",
    "    else:\n",
    "        table['epochAdam'] =  n_epochs\n",
    "        \n",
    "table['parameters'] =  cnn.count_params()\n",
    "\n",
    "for key in fittedHistory[-1].history:\n",
    "    if key in df.keys():\n",
    "        table[key] =  fittedHistory[-1].history[key][-1]\n",
    "        \n",
    "micros = int(round(time.time() * 1000000))\n",
    "# 100 times the prediction of test data\n",
    "for i in range(100):\n",
    "    preds = cnn.predict(testData)\n",
    "    \n",
    "micros = time.time() * 1000000 - micros\n",
    "# normalize the time for single prediction\n",
    "micros = micros/100/testData.shape[0]\n",
    "\n",
    "table['inference_time'] =  micros\n",
    "\n",
    "#Plot normalized confusion matrix\n",
    "functions.plot_confusion_matrix(preds, testLabel, classes=labelList, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "precision = cnn.evaluate(testData,  testLabel)\n",
    "print (\"Precision: \", round(precision[1]*100,2),\"%\")\n",
    "\n",
    "table['test_acc'] =  round(precision[1],4)\n",
    "\n",
    "\n",
    "df = df.append(table, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "name = 'SiSoInc'\n",
    "print(name)\n",
    "table = line\n",
    "table['name'] =  name\n",
    "table['augmented_set'] =  'No'\n",
    "inputData, inputLabel, testData, testLabel, validData, validLabel, augmentedData, augmentedLabel, validation_data, loss_weights = functions.modelSelection(name, dataset, labels)\n",
    "\n",
    "cnn = models.SiSoInception(inputData,inputLabel)\n",
    "\n",
    "fittedHistory = []\n",
    "\n",
    "for count, optimizer in enumerate(optimizers):\n",
    "    print('Using optimizer number ' + str(count))\n",
    "    cnn.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy', top3_acc ], loss_weights=loss_weights)\n",
    "    fittedHistory.append(cnn.fit(inputData, inputLabel,\n",
    "                         epochs=epochs[count],\n",
    "                         batch_size=round(inputData.shape[0]/400),\n",
    "                         shuffle=True,\n",
    "                         validation_data=validation_data,\n",
    "                         callbacks = [esCallBack]))\n",
    "\n",
    "functions.plotHistory(epochs, fittedHistory, 'Training History')\n",
    "\n",
    "for count, fitted in enumerate(fittedHistory):\n",
    "    n_epochs = len(fitted.history['loss'])\n",
    "    if count == 0:\n",
    "        table['epochSGD'] =  n_epochs\n",
    "    else:\n",
    "        table['epochAdam'] =  n_epochs\n",
    "\n",
    "table['feature'] =  choice\n",
    "table['delta'] =  useDelta\n",
    "table['parameters'] =  cnn.count_params()\n",
    "\n",
    "for key in fittedHistory[-1].history:\n",
    "    if key in df.keys():\n",
    "        table[key] =  fittedHistory[-1].history[key][-1]\n",
    "        \n",
    "micros = int(round(time.time() * 1000000))\n",
    "# 100 times the prediction of test data\n",
    "for i in range(100):\n",
    "    preds = cnn.predict(testData)\n",
    "    \n",
    "micros = time.time() * 1000000 - micros\n",
    "# normalize the time for single prediction\n",
    "micros = micros/100/testData.shape[0]\n",
    "\n",
    "table['inference_time'] =  micros\n",
    "\n",
    "#Plot normalized confusion matrix\n",
    "functions.plot_confusion_matrix(preds, testLabel, classes=labelList, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "precision = cnn.evaluate(testData,  testLabel)\n",
    "print (\"Precision: \", round(precision[1]*100,2),\"%\")\n",
    "\n",
    "table['test_acc'] =  round(precision[1],4)\n",
    "\n",
    "\n",
    "df = df.append(table, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "table['augmented_set'] =  'Yes'\n",
    "cnn = models.SiSoInception(inputData,inputLabel, dropout = 0.5)\n",
    "\n",
    "print('Adding augmented dataset')\n",
    "inputData, inputLabel = functions.meltData(inputData, augmentedData, inputLabel, augmentedLabel, 0.7)\n",
    "\n",
    "fittedHistory = []\n",
    "\n",
    "for count, optimizer in enumerate(optimizers):\n",
    "    print('Using optimizer number ' + str(count))\n",
    "    cnn.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy', top3_acc ], loss_weights=loss_weights)\n",
    "    fittedHistory.append(cnn.fit(inputData, inputLabel,\n",
    "                         epochs=epochs[count],\n",
    "                         batch_size=round(inputData.shape[0]/400),\n",
    "                         shuffle=True,\n",
    "                         validation_data=validation_data,\n",
    "                         callbacks = [esCallBack]))\n",
    "    \n",
    "functions.plotHistory(epochs, fittedHistory, 'Training History')\n",
    "\n",
    "for count, fitted in enumerate(fittedHistory):\n",
    "    n_epochs = len(fitted.history['loss'])\n",
    "    if count == 0:\n",
    "        table['epochSGD'] =  n_epochs\n",
    "    else:\n",
    "        table['epochAdam'] =  n_epochs\n",
    "\n",
    "table['parameters'] =  cnn.count_params()\n",
    "\n",
    "for key in fittedHistory[-1].history:\n",
    "    if key in df.keys():\n",
    "        table[key] =  fittedHistory[-1].history[key][-1]\n",
    "        \n",
    "micros = int(round(time.time() * 1000000))\n",
    "# 100 times the prediction of test data\n",
    "for i in range(100):\n",
    "    preds = cnn.predict(testData)\n",
    "    \n",
    "micros = time.time() * 1000000 - micros\n",
    "# normalize the time for single prediction\n",
    "micros = micros/100/testData.shape[0]\n",
    "\n",
    "table['inference_time'] =  micros\n",
    "\n",
    "#Plot normalized confusion matrix\n",
    "functions.plot_confusion_matrix(preds, testLabel, classes=labelList, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "precision = cnn.evaluate(testData,  testLabel)\n",
    "print (\"Precision: \", round(precision[1]*100,2),\"%\")\n",
    "\n",
    "table['test_acc'] =  round(precision[1],4)\n",
    "\n",
    "\n",
    "df = df.append(table, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "name = 'MiSoInc'\n",
    "print(name)\n",
    "table = line\n",
    "table['name'] =  name\n",
    "table['augmented_set'] =  'No'\n",
    "inputData, inputLabel, testData, testLabel, validData, validLabel, augmentedData, augmentedLabel, validation_data, loss_weights = functions.modelSelection(name, dataset, labels)\n",
    "\n",
    "cnn = models.MiSoInception(inputData, inputLabel, dropout = 0.4)\n",
    "\n",
    "fittedHistory = []\n",
    "\n",
    "for count, optimizer in enumerate(optimizers):\n",
    "    print('Using optimizer number ' + str(count))\n",
    "    cnn.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy', top3_acc ], loss_weights=loss_weights)\n",
    "    fittedHistory.append(cnn.fit(inputData, inputLabel,\n",
    "                         epochs=epochs[count],\n",
    "                         batch_size=round(inputData[0].shape[0]/400),\n",
    "                         shuffle=True,\n",
    "                         validation_data=validation_data,\n",
    "                         callbacks = [esCallBack]))\n",
    "\n",
    "functions.plotHistory(epochs, fittedHistory, 'Training History')\n",
    "\n",
    "for count, fitted in enumerate(fittedHistory):\n",
    "    n_epochs = len(fitted.history['loss'])\n",
    "    if count == 0:\n",
    "        table['epochSGD'] =  n_epochs\n",
    "    else:\n",
    "        table['epochAdam'] =  n_epochs\n",
    "\n",
    "table['feature'] =  choice\n",
    "table['delta'] =  useDelta\n",
    "table['parameters'] =  cnn.count_params()\n",
    "\n",
    "for key in fittedHistory[-1].history:\n",
    "    if key in df.keys():\n",
    "        table[key] =  fittedHistory[-1].history[key][-1]\n",
    "        \n",
    "micros = int(round(time.time() * 1000000))\n",
    "# 100 times the prediction of test data\n",
    "for i in range(100):\n",
    "    preds = cnn.predict(testData)\n",
    "    \n",
    "micros = time.time() * 1000000 - micros\n",
    "# normalize the time for single prediction\n",
    "micros = micros/100/testData[0].shape[0]\n",
    "\n",
    "table['inference_time'] =  micros\n",
    "\n",
    "#Plot normalized confusion matrix\n",
    "functions.plot_confusion_matrix(preds, testLabel, classes=labelList, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "precision = cnn.evaluate(testData,  testLabel)\n",
    "print (\"Precision: \", round(precision[1]*100,2),\"%\")\n",
    "\n",
    "table['test_acc'] =  round(precision[1],4)\n",
    "\n",
    "\n",
    "df = df.append(table, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "table['augmented_set'] =  'Yes'\n",
    "cnn = models.MiSoInception(inputData,inputLabel, dropout = 0.4)\n",
    "\n",
    "print('Adding augmented dataset')\n",
    "inputData, inputLabel = functions.meltData(inputData, augmentedData, inputLabel, augmentedLabel, 0.7)\n",
    "\n",
    "fittedHistory = []\n",
    "\n",
    "for count, optimizer in enumerate(optimizers):\n",
    "    print('Using optimizer number ' + str(count))\n",
    "    cnn.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy', top3_acc ], loss_weights=loss_weights)\n",
    "    fittedHistory.append(cnn.fit(inputData, inputLabel,\n",
    "                         epochs=epochs[count],\n",
    "                         batch_size=round(inputData[0].shape[0]/400),\n",
    "                         shuffle=True,\n",
    "                         validation_data=validation_data,\n",
    "                         callbacks = [esCallBack]))\n",
    "    \n",
    "functions.plotHistory(epochs, fittedHistory, 'Training History')\n",
    "\n",
    "for count, fitted in enumerate(fittedHistory):\n",
    "    n_epochs = len(fitted.history['loss'])\n",
    "    if count == 0:\n",
    "        table['epochSGD'] =  n_epochs\n",
    "    else:\n",
    "        table['epochAdam'] =  n_epochs\n",
    "\n",
    "table['parameters'] =  cnn.count_params()\n",
    "\n",
    "for key in fittedHistory[-1].history:\n",
    "    if key in df.keys():\n",
    "        table[key] =  fittedHistory[-1].history[key][-1]\n",
    "        \n",
    "micros = int(round(time.time() * 1000000))\n",
    "# 100 times the prediction of test data\n",
    "for i in range(100):\n",
    "    preds = cnn.predict(testData)\n",
    "    \n",
    "micros = time.time() * 1000000 - micros\n",
    "# normalize the time for single prediction\n",
    "micros = micros/100/testData[0].shape[0]\n",
    "\n",
    "table['inference_time'] =  micros\n",
    "\n",
    "#Plot normalized confusion matrix\n",
    "functions.plot_confusion_matrix(preds, testLabel, classes=labelList, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "precision = cnn.evaluate(testData,  testLabel)\n",
    "print (\"Precision: \", round(precision[1]*100,2),\"%\")\n",
    "\n",
    "table['test_acc'] =  round(precision[1],4)\n",
    "\n",
    "\n",
    "df = df.append(table, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('variables/dataFrame.pkl', 'wb') as f:  \n",
    "        pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
