{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook\n",
    "%matplotlib notebook\n",
    "## save variables\n",
    "import pickle\n",
    "## folder names\n",
    "from glob import glob\n",
    "## standard libraries\n",
    "import numpy as np\n",
    "\n",
    "##\n",
    "#from sklearn.decomposition import PCA\n",
    "#from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "#!{sys.executable} -m pip install tensorflow-gpu --user\n",
    "#!{sys.executable} -m pip install keras --user\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "import keras\n",
    "from keras.optimizers import SGD, Adam\n",
    "#!{sys.executable} -m pip install hyperas --user\n",
    "#!{sys.executable} -m pip install networkx==1.11 --user\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "\n",
    "import os.path\n",
    "import datetime\n",
    "\n",
    "from dependencies import models\n",
    "from dependencies import functions\n",
    "\n",
    "from dependencies.convnet_drawer.convnet_drawer import Model\n",
    "from dependencies.convnet_drawer.convnet_drawer import Conv2D\n",
    "from dependencies.convnet_drawer.convnet_drawer import MaxPooling2D as MaxPooling2D_drawer\n",
    "from dependencies.convnet_drawer.convnet_drawer import Flatten as Flatten_drawer\n",
    "from dependencies.convnet_drawer.convnet_drawer import Dense as Dense_drawer\n",
    "from dependencies.convnet_drawer.matplotlib_util import save_model_to_file\n",
    "from dependencies.convnet_drawer.keras_util import convert_drawer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import mfcc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded variables/mfccDict_DD[nC=14 wL=0.025 wS=0.01].pkl\n"
     ]
    }
   ],
   "source": [
    "#choose which dictionary to use\n",
    "choice =     'mfcc'#'logfilter' #\n",
    "useDelta =  True\n",
    "#retrieving of used values for the computation of mfcc\n",
    "with open('variables/mfccValues.pkl', 'rb') as f:  \n",
    "    values = pickle.load(f)\n",
    "selected = 0\n",
    "if choice == 'mfcc': \n",
    "    \n",
    "    \n",
    "    #name format of the selected data\n",
    "    if useDelta:\n",
    "        name = 'variables/mfccDict_DD[nC='+str(values[selected][0])+' wL='+str(values[selected][2])+' wS='+str(values[selected][3])+'].pkl'\n",
    "    else:\n",
    "        name = 'variables/mfccDict[nC='+str(values[selected][0])+' wL='+str(values[selected][2])+' wS='+str(values[selected][3])+'].pkl'\n",
    "    #loading in usedDict of the mfcc dict\n",
    "    #name = 'variables/mfcc_delta_deltadelta_PCA.pkl'\n",
    "    with open(name, 'rb') as f: \n",
    "        usedDict = pickle.load(f)\n",
    "    print('Loaded '+name)\n",
    "\n",
    "elif choice == 'logfilter':\n",
    "    if useDelta:\n",
    "        name = 'variables/logfiltDict_DD[nF='+str(values[selected][1])+' wL='+str(values[selected][2])+' wS='+str(values[selected][3])+'].pkl'\n",
    "    else:\n",
    "        name = 'variables/logfiltDict[nF='+str(values[selected][1])+' wL='+str(values[selected][2])+' wS='+str(values[selected][3])+'].pkl'\n",
    "    #name = 'variables/pcalogDict.pkl'\n",
    "    #saving in usedDict of the logfilter dict\n",
    "    with open(name, 'rb') as f:  \n",
    "        usedDict = pickle.load(f)\n",
    "    print('Loaded '+name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring and scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#core words of the dataset\n",
    "coreKey = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", \"zero\",\n",
    "           \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
    "\n",
    "#split of the core set\n",
    "numbers = ['one', 'two', 'three','four','five','six','seven','eight','nine', \"zero\"]\n",
    "\n",
    "words = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\"]\n",
    "\n",
    "test = [\"yes\", \"up\", \"down\", \"left\"]#, \"right\", \"on\", \"off\", \"stop\", \"go\", \"zero\",\n",
    "          # \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
    "\n",
    "#selecting the subset of words\n",
    "used = words\n",
    "\n",
    "used.append('silence')\n",
    "\n",
    "unknown = list(usedDict.keys())\n",
    "for key in used:\n",
    "    try:\n",
    "        unknown.remove(key)\n",
    "    except:\n",
    "        print(key, ' not in used')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'copy': True, 'quantile_range': (25.0, 75.0), 'with_centering': True, 'with_scaling': True}\n",
      "{'copy': True, 'quantile_range': (25.0, 75.0), 'with_centering': True, 'with_scaling': True}\n",
      "{'copy': True, 'quantile_range': (25.0, 75.0), 'with_centering': True, 'with_scaling': True}\n",
      "\n",
      "Wall time: 18.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#divding between train and test with also scaling data\n",
    "functions.train_test_creator(\n",
    "    {k: usedDict[k] for k in usedDict.keys() & used },\n",
    "    {k: usedDict[k] for k in usedDict.keys() & unknown },\n",
    "    with_unknown = False,\n",
    "    scalerType = 'robust',\n",
    "    depth = (len(usedDict[words[0]].shape)-3)*2 + 1 )\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "Jupyter.notebook.execute_cells([0])"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "from IPython.display import Javascript\n",
    "Javascript(\"Jupyter.notebook.execute_cells([0])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = functions.data()\n",
    "with open('variables/labelList.pkl', 'rb') as f: \n",
    "        labelList = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total 0.9 of the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "\n",
    "epoch = 40\n",
    "epochSGD = 20\n",
    "\n",
    "dest_directory = 'model_backup/'\n",
    "if not os.path.exists(dest_directory):\n",
    "      os.makedirs(dest_directory)\n",
    "\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn = models.model1(x_train,y_train, depth = 3)\n",
    "print(\"Model1\")\n",
    "cnn.summary()\n",
    "\n",
    "cnn = models.model2(x_train,y_train, depth = 3)\n",
    "print(\"Model2\")\n",
    "cnn.summary()\n",
    "\n",
    "cnn = models.model3(x_train,y_train, depth = 3)\n",
    "print(\"Model3\")\n",
    "cnn.summary()\n",
    "\n",
    "cnn = models.tinyDarknet(x_train,y_train, depth = 3)\n",
    "print(\"tiny darknet\")\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"MODEL1\")\n",
    "cnn = models.model1(x_train,y_train, depth = x_train.shape[3])\n",
    "\n",
    "cnn.summary()\n",
    "\n",
    "\n",
    "compiledAdam = cnn.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "fittedAdam = cnn.fit(x_train, y_train,\n",
    "                     epochs=epoch,\n",
    "                     validation_data=(x_test, y_test),\n",
    "                     batch_size=round(x_train.shape[0]/300),\n",
    "                     shuffle=True,\n",
    "                     callbacks = [tbCallBack])\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=0.0005, momentum=0.9, nesterov=True)\n",
    "compiledSGD = cnn.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "fittedSGD = cnn.fit(x_train, y_train, \n",
    "                epochs=epochSGD, \n",
    "                validation_data=(x_test, y_test), \n",
    "                batch_size=round(x_train.shape[0]/300), \n",
    "                shuffle=True,\n",
    "                callbacks = [tbCallBack])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = range(epoch)\n",
    "for key in fittedAdam.history:\n",
    "    ax.plot(x,fittedAdam.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = range(epochSGD)\n",
    "for key in fittedSGD.history:\n",
    "    ax.plot(x,fittedSGD.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "name = 'cnn1.bak'\n",
    "cnn.save(dest_directory + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"\\nMODEL3\")\n",
    "cnn = models.model3(x_train,y_train, depth = 3, basedim = 32)\n",
    "\n",
    "cnn.summary()\n",
    "\n",
    "\n",
    "compiledAdam = cnn.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "fittedAdam = cnn.fit(x_train, y_train,\n",
    "                     epochs=epoch,\n",
    "                     validation_data=(x_test, y_test),\n",
    "                     batch_size=round(x_train.shape[0]/200),\n",
    "                     shuffle=True,\n",
    "                     callbacks = [tbCallBack])\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=0.0005, momentum=0.9, nesterov=True)\n",
    "compiledSGD = cnn.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "fittedSGD = cnn.fit(x_train, y_train, \n",
    "                epochs=epochSGD, \n",
    "                validation_data=(x_test, y_test), \n",
    "                batch_size=round(x_train.shape[0]/200), \n",
    "                shuffle=True,\n",
    "                callbacks = [tbCallBack])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = range(epoch)\n",
    "for key in fittedAdam.history:\n",
    "    ax.plot(x,fittedAdam.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = range(epochSGD)\n",
    "for key in fittedSGD.history:\n",
    "    ax.plot(x,fittedSGD.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "name = 'cnn3.bak'\n",
    "cnn.save(dest_directory + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"\\nTINYDARKNET\")\n",
    "cnn = models.tinyDarknet(x_train,y_train, depth = 3, dropout = 0.05)\n",
    "#cnn.summary()\n",
    "\n",
    "\n",
    "compiledAdam = cnn.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "fittedAdam = cnn.fit(x_train, y_train,\n",
    "                     epochs=epoch,\n",
    "                     validation_data=(x_test, y_test),\n",
    "                     batch_size=round(x_train.shape[0]/200),\n",
    "                     shuffle=True,\n",
    "                     callbacks = [tbCallBack])\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=0.0005, momentum=0.9, nesterov=True)\n",
    "compiledSGD = cnn.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "fittedSGD = cnn.fit(x_train, y_train, \n",
    "                epochs=epochSGD, \n",
    "                validation_data=(x_test, y_test), \n",
    "                batch_size=round(x_train.shape[0]/200), \n",
    "                shuffle=True,\n",
    "                callbacks = [tbCallBack])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = range(epoch)\n",
    "for key in fittedAdam.history:\n",
    "    ax.plot(x,fittedAdam.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = range(epochSGD)\n",
    "for key in fittedSGD.history:\n",
    "    ax.plot(x,fittedSGD.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "name = 'cnn4.bak'\n",
    "cnn.save(dest_directory + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tensorboard --logdir Graph/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers import Flatten, Dense, Activation, BatchNormalization, Dropout, SpatialDropout2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def cBN(inputLayer, filt = 64, size = (1,1), padding = 'same', activation = 'relu', regu = 0.0, dropout = 0.05, strides = (1,1)):\n",
    "    cbn = Conv2D(filt, size, padding=padding, use_bias=False, kernel_regularizer=regularizers.l2(regu), strides = (1,1))(inputLayer)\n",
    "    cbn = Activation(activation)(cbn)\n",
    "    #cbn = BatchNormalization(epsilon=1e-05, momentum=0.1, axis=-1)(cbn)\n",
    "    return cbn\n",
    "\n",
    "def inception(inputLayer, filt = 64):\n",
    "    tower_1 = cBN(inputLayer, filt = filt)   \n",
    "    tower_1 = cBN(tower_1, size = (3,3), filt = filt)\n",
    "\n",
    "    tower_2 = cBN(inputLayer, filt = filt)\n",
    "    tower_2 = cBN(tower_2, size = (5,5), filt = filt)\n",
    "\n",
    "    tower_3 = MaxPooling2D((3,2), strides=(1,1), padding='same')(inputLayer)\n",
    "    tower_3 = cBN(tower_3, filt = filt)\n",
    "    \n",
    "    #tower_4 = cBN(inputLayer)\n",
    "\n",
    "    output = keras.layers.concatenate([tower_1, tower_2, tower_3], axis = 3)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_img = Input(name = 'input', shape = (x_train.shape[1], x_train.shape[2], x_train.shape[3]))\n",
    "\n",
    "output = inception(input_img, filt = 20)\n",
    "output = inception(input_img, filt = 30)\n",
    "\n",
    "output = MaxPooling2D(pool_size=(3,2), padding='same')(output)\n",
    "output = SpatialDropout2D(0.15)(output)\n",
    "\n",
    "output = inception(output,filt = 30)\n",
    "output = inception(output, filt = 40)\n",
    "\n",
    "output = MaxPooling2D(pool_size=(3,2), padding='same')(output)\n",
    "output = SpatialDropout2D(0.15)(output)\n",
    "\n",
    "output = inception(output, filt = 40)\n",
    "output = inception(output, filt = 60)\n",
    "\n",
    "output = MaxPooling2D(pool_size=(3,2), padding='same')(output)\n",
    "output = SpatialDropout2D(0.15)(output)\n",
    "\n",
    "output = inception(output, filt = 60)\n",
    "output = inception(output, filt = 100)\n",
    "\n",
    "output = GlobalAveragePooling2D()(output)\n",
    "#output = Dropout(0.2)(output)\n",
    "\n",
    "output = Dense(100)(output)\n",
    "#output = BatchNormalization(epsilon=1e-05, momentum=0.1)(output)\n",
    "output = Activation('relu')(output)\n",
    "output = Dropout(0.4)(output)\n",
    "output = Dense(y_train.shape[1], name = 'output2', activation='softmax')(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn = Model(inputs = input_img, outputs = output)\n",
    "validation_data=({'input': x_test}, {'output2': y_test})\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.001/epoch, amsgrad=True)\n",
    "compiledAdam = cnn.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=['categorical_accuracy'], loss_weights={'output2' : 1.})\n",
    "fittedAdam = cnn.fit(x_train,  y_train,\n",
    "                     epochs=epoch,\n",
    "                     batch_size=round(x_train.shape[0]/400),\n",
    "                     shuffle=True,\n",
    "                     validation_data=validation_data, \n",
    "                     callbacks = [tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.001, decay=0.001/epochSGD, momentum=0.9, nesterov=True)\n",
    "compiledSGD = cnn.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy'], loss_weights={'output2' : 1.})\n",
    "fittedSGD = cnn.fit(x_train, y_train,\n",
    "                     epochs=epochSGD,\n",
    "                     batch_size=round(x_train.shape[0]/400),\n",
    "                     validation_data=validation_data,\n",
    "                     shuffle=True,\n",
    "                     callbacks = [tbCallBack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_inception(inputLayer, filt = 64, base = 3):\n",
    "    tower_1 = cBN(inputLayer, filt = filt)   \n",
    "    tower_1 = cBN(tower_1, size = (base,1), filt = filt)\n",
    "\n",
    "    tower_2 = cBN(inputLayer, filt = filt)\n",
    "    tower_2 = cBN(tower_2, size = (base+1,1), filt = filt)\n",
    "\n",
    "    tower_3 = cBN(inputLayer, filt = filt)\n",
    "    tower_3 = cBN(tower_3, filt = filt, size = (base+2,1))\n",
    "    \n",
    "    output = keras.layers.concatenate([tower_1, tower_2, tower_3], axis = 3)\n",
    "    return output\n",
    "    \n",
    "def singleInput(x_train, name):\n",
    "    single_input = Input(name = name, shape = (x_train.shape[1], x_train.shape[2], 1))\n",
    "    output = Conv2D(120, (3,round(x_train.shape[2])))(single_input)\n",
    "    output = lin_inception(output, filt = 80, base = 3)\n",
    "    return single_input, output\n",
    "def firstConcat(x_train):\n",
    "    first, first_output = singleInput(x_train,'first')\n",
    "    second, second_output = singleInput(x_train,'second')\n",
    "    third, third_output = singleInput(x_train,'third')\n",
    "    concat = keras.layers.concatenate([first_output, second_output, third_output], axis = 3)\n",
    "    return first, second, third, concat\n",
    "\n",
    "first_input, second_input, third_input, concat = firstConcat(x_train)\n",
    "output = MaxPooling2D(pool_size=(3,1), padding='same')(concat)\n",
    "output = SpatialDropout2D(0.15)(output)\n",
    "\n",
    "output = lin_inception(output, filt = 80, base = 2)\n",
    "output = lin_inception(output, filt = 80, base = 3)\n",
    "'''\n",
    "output = MaxPooling2D(pool_size=(3,1), padding='same')(output)\n",
    "output = SpatialDropout2D(0.15)(output)\n",
    "\n",
    "output = lin_inception(output, filt = 50, base = 4)\n",
    "output = lin_inception(output, filt = 50, base = 5)\n",
    "\n",
    "output = MaxPooling2D(pool_size=(3,1), padding='same')(output)\n",
    "output = SpatialDropout2D(0.15)(output)\n",
    "\n",
    "output = lin_inception(output, filt = 50, base = 4)\n",
    "output = lin_inception(output, filt = 50, base = 5)\n",
    "\n",
    "output = GlobalAveragePooling2D()(output)\n",
    "'''\n",
    "output = AveragePooling2D((11,1))(output)\n",
    "\n",
    "#output = Flatten()(output)\n",
    "output = Dense(100)(output)\n",
    "output = Activation('relu')(output)\n",
    "output = Dropout(0.6)(output)\n",
    "\n",
    "output = Dense(y_train.shape[1], name = 'output', activation='softmax')(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "first (InputLayer)              (None, 99, 14, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "second (InputLayer)             (None, 99, 14, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "third (InputLayer)              (None, 99, 14, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_818 (Conv2D)             (None, 97, 1, 120)   5160        first[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_825 (Conv2D)             (None, 97, 1, 120)   5160        second[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_832 (Conv2D)             (None, 97, 1, 120)   5160        third[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_819 (Conv2D)             (None, 97, 1, 90)    10800       conv2d_818[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_821 (Conv2D)             (None, 97, 1, 90)    10800       conv2d_818[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_823 (Conv2D)             (None, 97, 1, 90)    10800       conv2d_818[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_826 (Conv2D)             (None, 97, 1, 90)    10800       conv2d_825[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_828 (Conv2D)             (None, 97, 1, 90)    10800       conv2d_825[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_830 (Conv2D)             (None, 97, 1, 90)    10800       conv2d_825[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_833 (Conv2D)             (None, 97, 1, 90)    10800       conv2d_832[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_835 (Conv2D)             (None, 97, 1, 90)    10800       conv2d_832[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_837 (Conv2D)             (None, 97, 1, 90)    10800       conv2d_832[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_774 (Activation)     (None, 97, 1, 90)    0           conv2d_819[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_776 (Activation)     (None, 97, 1, 90)    0           conv2d_821[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_778 (Activation)     (None, 97, 1, 90)    0           conv2d_823[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_780 (Activation)     (None, 97, 1, 90)    0           conv2d_826[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_782 (Activation)     (None, 97, 1, 90)    0           conv2d_828[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_784 (Activation)     (None, 97, 1, 90)    0           conv2d_830[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_786 (Activation)     (None, 97, 1, 90)    0           conv2d_833[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_788 (Activation)     (None, 97, 1, 90)    0           conv2d_835[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_790 (Activation)     (None, 97, 1, 90)    0           conv2d_837[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_820 (Conv2D)             (None, 97, 1, 90)    24300       activation_774[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_822 (Conv2D)             (None, 97, 1, 90)    32400       activation_776[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_824 (Conv2D)             (None, 97, 1, 90)    40500       activation_778[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_827 (Conv2D)             (None, 97, 1, 90)    24300       activation_780[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_829 (Conv2D)             (None, 97, 1, 90)    32400       activation_782[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_831 (Conv2D)             (None, 97, 1, 90)    40500       activation_784[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_834 (Conv2D)             (None, 97, 1, 90)    24300       activation_786[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_836 (Conv2D)             (None, 97, 1, 90)    32400       activation_788[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_838 (Conv2D)             (None, 97, 1, 90)    40500       activation_790[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_775 (Activation)     (None, 97, 1, 90)    0           conv2d_820[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_777 (Activation)     (None, 97, 1, 90)    0           conv2d_822[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_779 (Activation)     (None, 97, 1, 90)    0           conv2d_824[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_781 (Activation)     (None, 97, 1, 90)    0           conv2d_827[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_783 (Activation)     (None, 97, 1, 90)    0           conv2d_829[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_785 (Activation)     (None, 97, 1, 90)    0           conv2d_831[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_787 (Activation)     (None, 97, 1, 90)    0           conv2d_834[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_789 (Activation)     (None, 97, 1, 90)    0           conv2d_836[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_791 (Activation)     (None, 97, 1, 90)    0           conv2d_838[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_157 (Concatenate)   (None, 97, 1, 270)   0           activation_775[0][0]             \n",
      "                                                                 activation_777[0][0]             \n",
      "                                                                 activation_779[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_158 (Concatenate)   (None, 97, 1, 270)   0           activation_781[0][0]             \n",
      "                                                                 activation_783[0][0]             \n",
      "                                                                 activation_785[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_159 (Concatenate)   (None, 97, 1, 270)   0           activation_787[0][0]             \n",
      "                                                                 activation_789[0][0]             \n",
      "                                                                 activation_791[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_160 (Concatenate)   (None, 97, 1, 810)   0           concatenate_157[0][0]            \n",
      "                                                                 concatenate_158[0][0]            \n",
      "                                                                 concatenate_159[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_68 (MaxPooling2D) (None, 33, 1, 810)   0           concatenate_160[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_39 (SpatialDr (None, 33, 1, 810)   0           max_pooling2d_68[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_839 (Conv2D)             (None, 33, 1, 50)    40500       spatial_dropout2d_39[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_841 (Conv2D)             (None, 33, 1, 50)    40500       spatial_dropout2d_39[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_843 (Conv2D)             (None, 33, 1, 50)    40500       spatial_dropout2d_39[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_792 (Activation)     (None, 33, 1, 50)    0           conv2d_839[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_794 (Activation)     (None, 33, 1, 50)    0           conv2d_841[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_796 (Activation)     (None, 33, 1, 50)    0           conv2d_843[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_840 (Conv2D)             (None, 33, 1, 50)    5000        activation_792[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_842 (Conv2D)             (None, 33, 1, 50)    7500        activation_794[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_844 (Conv2D)             (None, 33, 1, 50)    10000       activation_796[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_793 (Activation)     (None, 33, 1, 50)    0           conv2d_840[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_795 (Activation)     (None, 33, 1, 50)    0           conv2d_842[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_797 (Activation)     (None, 33, 1, 50)    0           conv2d_844[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_161 (Concatenate)   (None, 33, 1, 150)   0           activation_793[0][0]             \n",
      "                                                                 activation_795[0][0]             \n",
      "                                                                 activation_797[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_845 (Conv2D)             (None, 33, 1, 50)    7500        concatenate_161[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_847 (Conv2D)             (None, 33, 1, 50)    7500        concatenate_161[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_849 (Conv2D)             (None, 33, 1, 50)    7500        concatenate_161[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_798 (Activation)     (None, 33, 1, 50)    0           conv2d_845[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_800 (Activation)     (None, 33, 1, 50)    0           conv2d_847[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_802 (Activation)     (None, 33, 1, 50)    0           conv2d_849[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_846 (Conv2D)             (None, 33, 1, 50)    7500        activation_798[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_848 (Conv2D)             (None, 33, 1, 50)    10000       activation_800[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_850 (Conv2D)             (None, 33, 1, 50)    12500       activation_802[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_799 (Activation)     (None, 33, 1, 50)    0           conv2d_846[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_801 (Activation)     (None, 33, 1, 50)    0           conv2d_848[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_803 (Activation)     (None, 33, 1, 50)    0           conv2d_850[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_162 (Concatenate)   (None, 33, 1, 150)   0           activation_799[0][0]             \n",
      "                                                                 activation_801[0][0]             \n",
      "                                                                 activation_803[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_24 (AveragePo (None, 3, 1, 150)    0           concatenate_162[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 3, 1, 100)    15100       average_pooling2d_24[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_804 (Activation)     (None, 3, 1, 100)    0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 3, 1, 100)    0           activation_804[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 3, 1, 11)     1111        dropout_23[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 616,991\n",
      "Trainable params: 616,991\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn = Model(inputs = [first_input, second_input, third_input], outputs = output)\n",
    "validation_data=({'first': x_test[:,:,:,0, np.newaxis], 'second': x_test[:,:,:,1, np.newaxis], 'third': x_test[:,:,:,2, np.newaxis]}, {'output': y_test})\n",
    "\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34836 samples, validate on 8710 samples\n",
      "Epoch 1/40\n",
      "34836/34836 [==============================] - 14s 410us/step - loss: 1.0319 - categorical_accuracy: 0.6530 - val_loss: 0.3518 - val_categorical_accuracy: 0.8874\n",
      "Epoch 2/40\n",
      "34836/34836 [==============================] - 11s 325us/step - loss: 0.3374 - categorical_accuracy: 0.8950 - val_loss: 0.2698 - val_categorical_accuracy: 0.9189\n",
      "Epoch 3/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.2423 - categorical_accuracy: 0.9269 - val_loss: 0.2179 - val_categorical_accuracy: 0.9341\n",
      "Epoch 4/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.2007 - categorical_accuracy: 0.9391 - val_loss: 0.2472 - val_categorical_accuracy: 0.9281\n",
      "Epoch 5/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.1689 - categorical_accuracy: 0.9481 - val_loss: 0.1841 - val_categorical_accuracy: 0.9421\n",
      "Epoch 6/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.1584 - categorical_accuracy: 0.9516 - val_loss: 0.2117 - val_categorical_accuracy: 0.9374\n",
      "Epoch 7/40\n",
      "34836/34836 [==============================] - 11s 325us/step - loss: 0.1375 - categorical_accuracy: 0.9572 - val_loss: 0.1797 - val_categorical_accuracy: 0.9487\n",
      "Epoch 8/40\n",
      "34836/34836 [==============================] - 11s 325us/step - loss: 0.1203 - categorical_accuracy: 0.9619 - val_loss: 0.2045 - val_categorical_accuracy: 0.9462\n",
      "Epoch 9/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.1236 - categorical_accuracy: 0.9611 - val_loss: 0.1636 - val_categorical_accuracy: 0.9507\n",
      "Epoch 10/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.1011 - categorical_accuracy: 0.9678 - val_loss: 0.1882 - val_categorical_accuracy: 0.9478\n",
      "Epoch 11/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.0956 - categorical_accuracy: 0.9708 - val_loss: 0.2048 - val_categorical_accuracy: 0.9442\n",
      "Epoch 12/40\n",
      "34836/34836 [==============================] - 11s 325us/step - loss: 0.0852 - categorical_accuracy: 0.9725 - val_loss: 0.1837 - val_categorical_accuracy: 0.9499\n",
      "Epoch 13/40\n",
      "34836/34836 [==============================] - 11s 325us/step - loss: 0.0813 - categorical_accuracy: 0.9746 - val_loss: 0.1974 - val_categorical_accuracy: 0.9533\n",
      "Epoch 14/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.0679 - categorical_accuracy: 0.9779 - val_loss: 0.1826 - val_categorical_accuracy: 0.9542\n",
      "Epoch 15/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.0717 - categorical_accuracy: 0.9771 - val_loss: 0.1897 - val_categorical_accuracy: 0.9527\n",
      "Epoch 16/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.0679 - categorical_accuracy: 0.9781 - val_loss: 0.1873 - val_categorical_accuracy: 0.9519\n",
      "Epoch 17/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.0633 - categorical_accuracy: 0.9794 - val_loss: 0.2115 - val_categorical_accuracy: 0.9541\n",
      "Epoch 18/40\n",
      "34836/34836 [==============================] - 11s 325us/step - loss: 0.0515 - categorical_accuracy: 0.9823 - val_loss: 0.2390 - val_categorical_accuracy: 0.9510\n",
      "Epoch 19/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.0587 - categorical_accuracy: 0.9814 - val_loss: 0.2475 - val_categorical_accuracy: 0.9482\n",
      "Epoch 20/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.0550 - categorical_accuracy: 0.9821 - val_loss: 0.2075 - val_categorical_accuracy: 0.9512\n",
      "Epoch 21/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.0538 - categorical_accuracy: 0.9827 - val_loss: 0.2474 - val_categorical_accuracy: 0.9479\n",
      "Epoch 22/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.0485 - categorical_accuracy: 0.9842 - val_loss: 0.2295 - val_categorical_accuracy: 0.9533\n",
      "Epoch 23/40\n",
      "34836/34836 [==============================] - 11s 325us/step - loss: 0.0432 - categorical_accuracy: 0.9857 - val_loss: 0.2243 - val_categorical_accuracy: 0.9537\n",
      "Epoch 24/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.0421 - categorical_accuracy: 0.9867 - val_loss: 0.2386 - val_categorical_accuracy: 0.9526\n",
      "Epoch 25/40\n",
      "34836/34836 [==============================] - 11s 323us/step - loss: 0.0390 - categorical_accuracy: 0.9876 - val_loss: 0.2468 - val_categorical_accuracy: 0.9544\n",
      "Epoch 26/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.0389 - categorical_accuracy: 0.9879 - val_loss: 0.2498 - val_categorical_accuracy: 0.9520\n",
      "Epoch 27/40\n",
      "34836/34836 [==============================] - 11s 323us/step - loss: 0.0354 - categorical_accuracy: 0.9887 - val_loss: 0.2585 - val_categorical_accuracy: 0.9532\n",
      "Epoch 28/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.0382 - categorical_accuracy: 0.9883 - val_loss: 0.3003 - val_categorical_accuracy: 0.9454\n",
      "Epoch 29/40\n",
      "34836/34836 [==============================] - 11s 325us/step - loss: 0.0391 - categorical_accuracy: 0.9877 - val_loss: 0.3101 - val_categorical_accuracy: 0.9466\n",
      "Epoch 30/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.0285 - categorical_accuracy: 0.9913 - val_loss: 0.2649 - val_categorical_accuracy: 0.9542\n",
      "Epoch 31/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.0300 - categorical_accuracy: 0.9902 - val_loss: 0.2630 - val_categorical_accuracy: 0.9536\n",
      "Epoch 32/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.0231 - categorical_accuracy: 0.9923 - val_loss: 0.2484 - val_categorical_accuracy: 0.9532\n",
      "Epoch 33/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.0270 - categorical_accuracy: 0.9919 - val_loss: 0.2796 - val_categorical_accuracy: 0.9528\n",
      "Epoch 34/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.0207 - categorical_accuracy: 0.9931 - val_loss: 0.2780 - val_categorical_accuracy: 0.9529\n",
      "Epoch 35/40\n",
      "34836/34836 [==============================] - 11s 323us/step - loss: 0.0298 - categorical_accuracy: 0.9909 - val_loss: 0.2637 - val_categorical_accuracy: 0.9505\n",
      "Epoch 36/40\n",
      "34836/34836 [==============================] - 11s 325us/step - loss: 0.0217 - categorical_accuracy: 0.9935 - val_loss: 0.2625 - val_categorical_accuracy: 0.9568\n",
      "Epoch 37/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.0182 - categorical_accuracy: 0.9946 - val_loss: 0.2773 - val_categorical_accuracy: 0.9557\n",
      "Epoch 38/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.0159 - categorical_accuracy: 0.9952 - val_loss: 0.3102 - val_categorical_accuracy: 0.9526\n",
      "Epoch 39/40\n",
      "34836/34836 [==============================] - 11s 323us/step - loss: 0.0247 - categorical_accuracy: 0.9924 - val_loss: 0.2800 - val_categorical_accuracy: 0.9489\n",
      "Epoch 40/40\n",
      "34836/34836 [==============================] - 11s 324us/step - loss: 0.0200 - categorical_accuracy: 0.9945 - val_loss: 0.2958 - val_categorical_accuracy: 0.9515\n"
     ]
    }
   ],
   "source": [
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.001/epoch, amsgrad=True)\n",
    "compiledAdam = cnn.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=['categorical_accuracy'], loss_weights={'output' : 1.})\n",
    "fittedAdam = cnn.fit([x_train[:,:,:,0, np.newaxis], x_train[:,:,:,1, np.newaxis], x_train[:,:,:,2, np.newaxis]],  y_train,\n",
    "                     epochs=epoch,\n",
    "                     batch_size=round(x_train.shape[0]/400),\n",
    "                     shuffle=True,\n",
    "                     validation_data=validation_data, \n",
    "                     callbacks = [tbCallBack])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraClassifier(name, inputLayer, outputShape):\n",
    "    output0 = GlobalAveragePooling2D()(inputLayer)\n",
    "    output0 = Dense(100, activation = 'relu')(output0)\n",
    "    output0 = Dropout(0.7)(output0)\n",
    "    output0 = Dense( outputShape, name = name, activation='softmax')(output0)\n",
    "    return output0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(name = 'input', shape = (x_train.shape[1], x_train.shape[2], x_train.shape[3]))\n",
    "\n",
    "output = cBN(input_img)\n",
    "output = cBN(output, size = (3,3))\n",
    "output = cBN(output)\n",
    "output = cBN(output, size = (3,3))\n",
    "\n",
    "output = MaxPooling2D(pool_size=(3,2), padding='same')(output)\n",
    "\n",
    "output = inception(output)\n",
    "output0 = extraClassifier('output0', output, y_train.shape[1])\n",
    "output = inception(output)\n",
    "\n",
    "output = MaxPooling2D(pool_size=(3,2), padding='same')(output)\n",
    "\n",
    "output = inception(output)\n",
    "output1 = extraClassifier('output1',output, y_train.shape[1])\n",
    "output = inception(output)\n",
    "\n",
    "output = cBN(output)\n",
    "output = GlobalAveragePooling2D()(output)\n",
    "\n",
    "#output = Dense(90, activation = 'relu')(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output2 = Dense(y_train.shape[1], name = 'output2', activation='softmax')(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Model(inputs = input_img, outputs = [output2, output1, output0])\n",
    "validation_data=({'input': x_test}, {'output0': y_test, 'output1': y_test, 'output2': y_test})\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "compiledAdam = cnn.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=['accuracy'], loss_weights={'output0': 1., 'output1': 1., 'output2' : 1.})\n",
    "fittedAdam = cnn.fit(x_train, [y_train, y_train, y_train],\n",
    "                     epochs=epoch,\n",
    "                     batch_size=round(x_train.shape[0]/400),\n",
    "                     shuffle=True,\n",
    "                     validation_data=validation_data, \n",
    "                     callbacks = [tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.005, decay=0.0005, momentum=0.9, nesterov=True)\n",
    "compiledSGD = cnn.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy'], loss_weights={'output0': 0.4, 'output1': 0.7, 'output2' : 1.})\n",
    "fittedSGD = cnn.fit(x_train, [y_train, y_train, y_train],\n",
    "                     epochs=epochSGD,\n",
    "                     batch_size=round(x_train.shape[0]/400),\n",
    "                     validation_data=validation_data,\n",
    "                     shuffle=True,\n",
    "                     callbacks = [tbCallBack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dest_directory = 'model_backup/'\n",
    "name = 'cnn.bak'\n",
    "\n",
    "cnn = load_model(dest_directory + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#find prediction with test data\n",
    "preds = cnn.predict(x_test)\n",
    "#print(list(used))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion matrix\n",
    "functions.plotConfusionMatrix(preds[0],y_test,list(used))\n",
    "precision = cnn.evaluate(x_test,[y_test,y_test,y_test])\n",
    "#print (\"Precision: \", round(precision*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = range(epoch)\n",
    "for key in fittedAdam.history:\n",
    "    ax.plot(x,fittedAdam.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output of conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_copy = Sequential()\n",
    "cnn_copy.add(cnn.layers[0])\n",
    "result = cnn_copy.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(result[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_directory = 'model_backup/'\n",
    "if not os.path.exists(dest_directory):\n",
    "      os.makedirs(dest_directory)\n",
    "name = 'cnn.bak'\n",
    "cnn.save(dest_directory + name)\n",
    "\n",
    "#bak = load_model(dest_directory + name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_shape=(x_train.shape[1], x_train.shape[2],1))\n",
    "model.add(Conv2D(100, (4,4),  strides = (1,1), padding=\"valid\"))\n",
    "model.add(Conv2D(100, (4,2),  strides = (1,1), padding=\"valid\"))\n",
    "model.add(MaxPooling2D_drawer(pool_size=(3,3)))\n",
    "model.add(Conv2D(128, (4,2),  strides = (1,1), padding=\"valid\"))\n",
    "model.add(Conv2D(128, (5,2),  strides = (1,1), padding=\"valid\"))\n",
    "model.add(MaxPooling2D_drawer(pool_size=(4,1)))\n",
    "model.add(Flatten_drawer())\n",
    "model.add(Dense_drawer(100))\n",
    "model.add(Dense_drawer(y_train.shape[1]))\n",
    "\n",
    "#save to pdf\n",
    "save_model_to_file(model, \"example.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking for hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.activations import softmax\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, Convolution2D, MaxPooling2D, AveragePooling2D, BatchNormalization\n",
    "trials = Trials()\n",
    "best_run, best_model = optim.minimize(model=functions.create_model, \n",
    "                                      data=functions.data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=100,\n",
    "                                      trials=trials,\n",
    "                                      notebook_name='Processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_directory = 'model_backup/'\n",
    "'''\n",
    "best_model = load_model(dest_directory + 'best_model.bak')\n",
    "\n",
    "with open(dest_directory+'best_run.pkl', 'rb') as f:  \n",
    "    best_run = pickle.load(f)    \n",
    "'''\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(x_test, y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)\n",
    "best_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "dest_directory_temp =dest_directory + 'bestModel('+now.strftime(\"%m-%d %H.%M\")+\")\"\n",
    "if not os.path.exists(dest_directory_temp):\n",
    "      os.makedirs(dest_directory_temp)\n",
    "best_model.save(dest_directory_temp + '/best_model.bak')\n",
    "\n",
    "with open(dest_directory_temp + '/best_run.pkl', 'wb') as f:  \n",
    "    pickle.dump(best_run, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.best_trial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
