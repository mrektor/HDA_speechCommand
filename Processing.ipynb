{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook\n",
    "%matplotlib notebook\n",
    "## save variables\n",
    "import pickle\n",
    "## folder names\n",
    "from glob import glob\n",
    "## standard libraries\n",
    "import numpy as np\n",
    "\n",
    "##\n",
    "#from sklearn.decomposition import PCA\n",
    "#from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "#!{sys.executable} -m pip install tensorflow-gpu --user\n",
    "#!{sys.executable} -m pip install keras --user\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "import keras\n",
    "from keras.optimizers import SGD\n",
    "#!{sys.executable} -m pip install hyperas --user\n",
    "#!{sys.executable} -m pip install networkx==1.11 --user\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "\n",
    "import os.path\n",
    "import datetime\n",
    "\n",
    "from dependencies import models\n",
    "from dependencies import functions\n",
    "\n",
    "from dependencies.convnet_drawer.convnet_drawer import Model\n",
    "from dependencies.convnet_drawer.convnet_drawer import Conv2D\n",
    "from dependencies.convnet_drawer.convnet_drawer import MaxPooling2D as MaxPooling2D_drawer\n",
    "from dependencies.convnet_drawer.convnet_drawer import Flatten as Flatten_drawer\n",
    "from dependencies.convnet_drawer.convnet_drawer import Dense as Dense_drawer\n",
    "from dependencies.convnet_drawer.matplotlib_util import save_model_to_file\n",
    "from dependencies.convnet_drawer.keras_util import convert_drawer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import mfcc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded variables/mfccDict[nC=14 wL=0.025 wS=0.01].pkl\n"
     ]
    }
   ],
   "source": [
    "#choose which dictionary to use\n",
    "choice = 'mfcc'#'spectro'\n",
    "\n",
    "\n",
    "selected = 0\n",
    "if choice == 'mfcc':\n",
    "    \n",
    "    #retrieving of used values for the computation of mfcc\n",
    "    with open('variables/mfccValues.pkl', 'rb') as f:  \n",
    "        values = pickle.load(f)\n",
    "    \n",
    "    #name format of the selected data\n",
    "    name = 'variables/mfccDict[nC='+str(values[selected][0])+' wL='+str(values[selected][2])+' wS='+str(values[selected][3])+'].pkl'\n",
    "    \n",
    "    #saving in usedDict of the mfcc dict\n",
    "    with open(name, 'rb') as f: \n",
    "        usedDict = pickle.load(f)\n",
    "    print('Loaded '+name)\n",
    "\n",
    "elif choice == 'spectro':\n",
    "    \n",
    "    #saving in usedDict of the spectro dict\n",
    "    with open('variables/spectroDict.pkl', 'rb') as f:  \n",
    "        usedDict = pickle.load(f)\n",
    "    print('Loaded spectroram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring and scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#core words of the dataset\n",
    "coreKey = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", \"zero\",\n",
    "           \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
    "\n",
    "#split of the core set\n",
    "numbers = ['one', 'two', 'three','four','five','six','seven','eight','nine', \"zero\"]\n",
    "words = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\"]\n",
    "\n",
    "test = [\"yes\", \"up\", \"down\", \"left\"]#, \"right\", \"on\", \"off\", \"stop\", \"go\", \"zero\",\n",
    "          # \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
    "\n",
    "#selecting the subset of words\n",
    "used = words\n",
    "used.append('silence')\n",
    "unknown = list(usedDict.keys())\n",
    "for key in used:\n",
    "    try:\n",
    "        unknown.remove(key)\n",
    "    except:\n",
    "        print(key, ' not in used')\n",
    "\n",
    "#divding between train and test with also scaling data\n",
    "x_train, y_train, x_test, y_test, labelList = functions.train_test_creator(\n",
    "    {k: usedDict[k] for k in usedDict.keys() & used },\n",
    "    {k: usedDict[k] for k in usedDict.keys() & unknown },\n",
    "    with_unknown = True,\n",
    "    scalerType = 'robust')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    " \n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    " \n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    " \n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 93, 11, 64)        1856      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 93, 11, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 5, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 31, 5, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 4, 128)        65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 28, 4, 128)        512       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 25, 2, 256)        393472    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 25, 2, 256)        1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 25, 2, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               256100    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                1212      \n",
      "=================================================================\n",
      "Total params: 720,496\n",
      "Trainable params: 719,400\n",
      "Non-trainable params: 1,096\n",
      "_________________________________________________________________\n",
      "Model2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 94, 11, 128)       3200      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 94, 11, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 94, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 94, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 90, 9, 128)        245888    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 90, 9, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 90, 9, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 30, 3, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 3, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 2, 128)        98432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 28, 2, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 2, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 28, 2, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 25, 1, 256)        262400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 25, 1, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 25, 1, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 6, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               153700    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 12)                1212      \n",
      "=================================================================\n",
      "Total params: 767,392\n",
      "Trainable params: 766,112\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Model3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 96, 13, 16)        144       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 96, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 96, 13, 32)        2080      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 96, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 96, 13, 64)        8256      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 96, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 96, 13, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 24, 6, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 24, 6, 64)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 24, 6, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 24, 6, 32)         8224      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 24, 6, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 24, 6, 64)         8256      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 24, 6, 64)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 24, 6, 64)         256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 12, 3, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12, 3, 64)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 12, 3, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 12, 3, 128)        32896     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 12, 3, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 3, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 3, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 80)                30800     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 80)                320       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 12)                972       \n",
      "=================================================================\n",
      "Total params: 92,972\n",
      "Trainable params: 92,300\n",
      "Non-trainable params: 672\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiny darknet\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 99, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 99, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 99, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 99, 14, 16)        528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 99, 14, 16)        64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 99, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 99, 14, 64)        9280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 99, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 99, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 49, 7, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 49, 7, 16)         1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 49, 7, 16)         64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 49, 7, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 49, 7, 128)        18560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 49, 7, 128)        512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 49, 7, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 49, 7, 16)         2064      \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 49, 7, 16)         64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 49, 7, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 49, 7, 128)        18560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 49, 7, 128)        512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 49, 7, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 24, 3, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 24, 3, 32)         4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 24, 3, 32)         128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 24, 3, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 24, 3, 256)        73984     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 24, 3, 256)        1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 24, 3, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 24, 3, 32)         8224      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 24, 3, 32)         128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 24, 3, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 24, 3, 256)        73984     \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 24, 3, 256)        1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 24, 3, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 12, 1, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 12, 1, 64)         16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 12, 1, 64)         256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 12, 1, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 12, 1, 512)        66048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 12, 1, 512)        2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 12, 1, 512)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 12, 1, 64)         32832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 12, 1, 64)         256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 12, 1, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 12, 1, 512)        66048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 12, 1, 512)        2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 12, 1, 512)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 12, 1, 64)         32832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 12, 1, 64)         256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 12, 1, 64)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 4, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 80)                20560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 80)                320       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 12)                972       \n",
      "=================================================================\n",
      "Total params: 455,500\n",
      "Trainable params: 450,956\n",
      "Non-trainable params: 4,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn = models.model1(x_train,y_train)\n",
    "print(\"Model1\")\n",
    "cnn.summary()\n",
    "\n",
    "cnn = models.model2(x_train,y_train)\n",
    "print(\"Model2\")\n",
    "cnn.summary()\n",
    "\n",
    "cnn = models.model3(x_train,y_train)\n",
    "print(\"Model3\")\n",
    "cnn.summary()\n",
    "\n",
    "cnn = models.tinyDarknet(x_train,y_train)\n",
    "print(\"tiny darknet\")\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 40\n",
    "\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"MODEL1\")\n",
    "cnn = models.model1(x_train,y_train)\n",
    "\n",
    "cnn.summary()\n",
    "\n",
    "\n",
    "compiledAdam = cnn.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "fittedAdam = cnn.fit(x_train, y_train,\n",
    "                     epochs=epoch,\n",
    "                     validation_data=(x_test, y_test),\n",
    "                     batch_size=round(x_train.shape[0]/100),\n",
    "                     shuffle=True,\n",
    "                     callbacks = [tbCallBack])\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=0.0005, momentum=0.9, nesterov=True)\n",
    "compiledSGD = cnn.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "fittedSGD = cnn.fit(x_train, y_train, \n",
    "                epochs=epoch, \n",
    "                validation_data=(x_test, y_test), \n",
    "                batch_size=round(x_train.shape[0]/100), \n",
    "                shuffle=True,\n",
    "                callbacks = [tbCallBack])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = range(epoch)\n",
    "for key in fittedAdam.history:\n",
    "    ax.plot(x,fittedAdam.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = range(epoch)\n",
    "for key in fittedSGD.history:\n",
    "    ax.plot(x,fittedSGD.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MODEL2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_30 (Conv2D)           (None, 94, 11, 128)       3200      \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 94, 11, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 94, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 94, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 90, 9, 128)        245888    \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 90, 9, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 90, 9, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 30, 3, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 30, 3, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 28, 2, 128)        98432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 28, 2, 128)        512       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 28, 2, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 28, 2, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 25, 1, 256)        262400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 25, 1, 256)        1024      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 25, 1, 256)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 6, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 6, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               153700    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 12)                1212      \n",
      "=================================================================\n",
      "Total params: 767,392\n",
      "Trainable params: 766,112\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "Train on 40451 samples, validate on 10113 samples\n",
      "Epoch 1/40\n",
      "40451/40451 [==============================] - 36s 897us/step - loss: 2.4400 - acc: 0.1817 - val_loss: 3.4808 - val_acc: 0.1892\n",
      "Epoch 2/40\n",
      "40451/40451 [==============================] - 34s 845us/step - loss: 2.1037 - acc: 0.2601 - val_loss: 1.6610 - val_acc: 0.3958\n",
      "Epoch 3/40\n",
      "40451/40451 [==============================] - 34s 849us/step - loss: 1.8217 - acc: 0.3382 - val_loss: 1.8449 - val_acc: 0.4091\n",
      "Epoch 4/40\n",
      "40451/40451 [==============================] - 34s 851us/step - loss: 1.5942 - acc: 0.4139 - val_loss: 1.4213 - val_acc: 0.5095\n",
      "Epoch 5/40\n",
      "40451/40451 [==============================] - 34s 850us/step - loss: 1.4242 - acc: 0.4790 - val_loss: 1.9679 - val_acc: 0.5267\n",
      "Epoch 6/40\n",
      "40451/40451 [==============================] - 35s 857us/step - loss: 1.2953 - acc: 0.5308 - val_loss: 1.4769 - val_acc: 0.6101\n",
      "Epoch 7/40\n",
      "40451/40451 [==============================] - 34s 849us/step - loss: 1.2000 - acc: 0.5699 - val_loss: 1.0044 - val_acc: 0.6625\n",
      "Epoch 8/40\n",
      "40451/40451 [==============================] - 35s 855us/step - loss: 1.1224 - acc: 0.6011 - val_loss: 0.9713 - val_acc: 0.6972\n",
      "Epoch 9/40\n",
      "40451/40451 [==============================] - 35s 857us/step - loss: 1.0663 - acc: 0.6282 - val_loss: 0.8647 - val_acc: 0.7426\n",
      "Epoch 10/40\n",
      "40451/40451 [==============================] - 35s 856us/step - loss: 0.9902 - acc: 0.6562 - val_loss: 0.6550 - val_acc: 0.7970\n",
      "Epoch 11/40\n",
      "40451/40451 [==============================] - 35s 855us/step - loss: 0.9514 - acc: 0.6711 - val_loss: 0.7112 - val_acc: 0.7902\n",
      "Epoch 12/40\n",
      "40451/40451 [==============================] - 34s 851us/step - loss: 0.9033 - acc: 0.6903 - val_loss: 0.6567 - val_acc: 0.7942\n",
      "Epoch 13/40\n",
      "40451/40451 [==============================] - 34s 853us/step - loss: 0.8572 - acc: 0.7072 - val_loss: 0.6362 - val_acc: 0.7990\n",
      "Epoch 14/40\n",
      "40451/40451 [==============================] - 34s 850us/step - loss: 0.8366 - acc: 0.7187 - val_loss: 0.5735 - val_acc: 0.8134\n",
      "Epoch 15/40\n",
      "40451/40451 [==============================] - 35s 855us/step - loss: 0.8005 - acc: 0.7309 - val_loss: 0.5145 - val_acc: 0.8433\n",
      "Epoch 16/40\n",
      "40451/40451 [==============================] - 34s 852us/step - loss: 0.7713 - acc: 0.7406 - val_loss: 0.6598 - val_acc: 0.8039\n",
      "Epoch 17/40\n",
      "40451/40451 [==============================] - 34s 853us/step - loss: 0.7495 - acc: 0.7537 - val_loss: 0.5227 - val_acc: 0.8296\n",
      "Epoch 18/40\n",
      "40451/40451 [==============================] - 35s 854us/step - loss: 0.7295 - acc: 0.7570 - val_loss: 0.4849 - val_acc: 0.8396\n",
      "Epoch 19/40\n",
      "40451/40451 [==============================] - 34s 852us/step - loss: 0.7119 - acc: 0.7643 - val_loss: 0.5034 - val_acc: 0.8492\n",
      "Epoch 20/40\n",
      "40451/40451 [==============================] - 34s 850us/step - loss: 0.6881 - acc: 0.7721 - val_loss: 0.5534 - val_acc: 0.8308\n",
      "Epoch 21/40\n",
      "40451/40451 [==============================] - 34s 851us/step - loss: 0.6806 - acc: 0.7760 - val_loss: 0.4999 - val_acc: 0.8355\n",
      "Epoch 22/40\n",
      "40451/40451 [==============================] - 34s 850us/step - loss: 0.6619 - acc: 0.7843 - val_loss: 0.4525 - val_acc: 0.8514\n",
      "Epoch 23/40\n",
      "40451/40451 [==============================] - 34s 847us/step - loss: 0.6364 - acc: 0.7902 - val_loss: 0.4435 - val_acc: 0.8569\n",
      "Epoch 24/40\n",
      "40451/40451 [==============================] - 35s 854us/step - loss: 0.6276 - acc: 0.7944 - val_loss: 0.4245 - val_acc: 0.8724\n",
      "Epoch 25/40\n",
      "40451/40451 [==============================] - 35s 856us/step - loss: 0.6156 - acc: 0.7979 - val_loss: 0.4395 - val_acc: 0.8632\n",
      "Epoch 26/40\n",
      "40451/40451 [==============================] - 34s 853us/step - loss: 0.6057 - acc: 0.8013 - val_loss: 0.4276 - val_acc: 0.8718\n",
      "Epoch 27/40\n",
      "40451/40451 [==============================] - 35s 854us/step - loss: 0.5920 - acc: 0.8085 - val_loss: 0.4030 - val_acc: 0.8689\n",
      "Epoch 28/40\n",
      "23760/40451 [================>.............] - ETA: 13s - loss: 0.5627 - acc: 0.8181"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"\\nMODEL2\")\n",
    "\n",
    "cnn = models.model2(x_train,y_train)\n",
    "\n",
    "cnn.summary()\n",
    "\n",
    "\n",
    "compiledAdam = cnn.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "fittedAdam = cnn.fit(x_train, y_train,\n",
    "                     epochs=epoch,\n",
    "                     validation_data=(x_test, y_test),\n",
    "                     batch_size=round(x_train.shape[0]/150),\n",
    "                     shuffle=True,\n",
    "                     callbacks = [tbCallBack])\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=0.0005, momentum=0.9, nesterov=True)\n",
    "compiledSGD = cnn.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "fittedSGD = cnn.fit(x_train, y_train, \n",
    "                epochs=epoch, \n",
    "                validation_data=(x_test, y_test), \n",
    "                batch_size=round(x_train.shape[0]/150), \n",
    "                shuffle=True,\n",
    "                callbacks = [tbCallBack])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = range(epoch)\n",
    "for key in fittedAdam.history:\n",
    "    ax.plot(x,fittedAdam.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = range(epoch)\n",
    "for key in fittedSGD.history:\n",
    "    ax.plot(x,fittedSGD.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"\\nMODEL3\")\n",
    "cnn = models.model3(x_train,y_train)\n",
    "\n",
    "cnn.summary()\n",
    "\n",
    "\n",
    "compiledAdam = cnn.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "fittedAdam = cnn.fit(x_train, y_train,\n",
    "                     epochs=epoch,\n",
    "                     validation_data=(x_test, y_test),\n",
    "                     batch_size=round(x_train.shape[0]/100),\n",
    "                     shuffle=True,\n",
    "                     callbacks = [tbCallBack])\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=0.0005, momentum=0.9, nesterov=True)\n",
    "compiledSGD = cnn.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "fittedSGD = cnn.fit(x_train, y_train, \n",
    "                epochs=epoch, \n",
    "                validation_data=(x_test, y_test), \n",
    "                batch_size=round(x_train.shape[0]/100), \n",
    "                shuffle=True,\n",
    "                callbacks = [tbCallBack])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = range(epoch)\n",
    "for key in fittedAdam.history:\n",
    "    ax.plot(x,fittedAdam.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = range(epoch)\n",
    "for key in fittedSGD.history:\n",
    "    ax.plot(x,fittedSGD.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"\\nTINYDARKNET\")\n",
    "cnn = models.tinyDarknet(x_train,y_train)\n",
    "cnn.summary()\n",
    "\n",
    "\n",
    "compiledAdam = cnn.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "fittedAdam = cnn.fit(x_train, y_train,\n",
    "                     epochs=epoch,\n",
    "                     validation_data=(x_test, y_test),\n",
    "                     batch_size=round(x_train.shape[0]/150),\n",
    "                     shuffle=True,\n",
    "                     callbacks = [tbCallBack])\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=0.0005, momentum=0.9, nesterov=True)\n",
    "compiledSGD = cnn.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "fittedSGD = cnn.fit(x_train, y_train, \n",
    "                epochs=epoch, \n",
    "                validation_data=(x_test, y_test), \n",
    "                batch_size=round(x_train.shape[0]/150), \n",
    "                shuffle=True,\n",
    "                callbacks = [tbCallBack])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = range(epoch)\n",
    "for key in fittedAdam.history:\n",
    "    ax.plot(x,fittedAdam.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = range(epoch)\n",
    "for key in fittedSGD.history:\n",
    "    ax.plot(x,fittedSGD.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tensorboard --logdir Graph/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dest_directory = 'model_backup/'\n",
    "name = 'cnn.bak'\n",
    "\n",
    "cnn = load_model(dest_directory + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#find prediction with test data\n",
    "preds = cnn.predict(x_test)\n",
    "print(list(used))\n",
    "\n",
    "#plot confusion matrix\n",
    "functions.plotConfusionMatrix(preds,y_test,list(used))\n",
    "loss, precision = cnn.evaluate(x_test,y_test)\n",
    "print (\"Precision: \", round(precision*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = range(epoch)\n",
    "for key in fittedAdam.history:\n",
    "    ax.plot(x,fittedAdam.history[key],label=key)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "for label in legend.get_texts():\n",
    "    label.set_fontsize('large')\n",
    "\n",
    "for label in legend.get_lines():\n",
    "    label.set_linewidth(1.5)  # the legend line width\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output of conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_copy = Sequential()\n",
    "cnn_copy.add(cnn.layers[0])\n",
    "result = cnn_copy.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(result[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_directory = 'model_backup/'\n",
    "if not os.path.exists(dest_directory):\n",
    "      os.makedirs(dest_directory)\n",
    "name = 'cnn.bak'\n",
    "cnn.save(dest_directory + name)\n",
    "\n",
    "#bak = load_model(dest_directory + name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_shape=(x_train.shape[1], x_train.shape[2],1))\n",
    "model.add(Conv2D(100, (4,4),  strides = (1,1), padding=\"valid\"))\n",
    "model.add(Conv2D(100, (4,2),  strides = (1,1), padding=\"valid\"))\n",
    "model.add(MaxPooling2D_drawer(pool_size=(3,3)))\n",
    "model.add(Conv2D(128, (4,2),  strides = (1,1), padding=\"valid\"))\n",
    "model.add(Conv2D(128, (5,2),  strides = (1,1), padding=\"valid\"))\n",
    "model.add(MaxPooling2D_drawer(pool_size=(4,1)))\n",
    "model.add(Flatten_drawer())\n",
    "model.add(Dense_drawer(100))\n",
    "model.add(Dense_drawer(y_train.shape[1]))\n",
    "\n",
    "#save to pdf\n",
    "save_model_to_file(model, \"example.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking for hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.activations import softmax\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, Convolution2D, MaxPooling2D, AveragePooling2D, BatchNormalization\n",
    "trials = Trials()\n",
    "best_run, best_model = optim.minimize(model=functions.create_model, \n",
    "                                      data=functions.data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=100,\n",
    "                                      trials=trials,\n",
    "                                      notebook_name='Processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_directory = 'model_backup/'\n",
    "'''\n",
    "best_model = load_model(dest_directory + 'best_model.bak')\n",
    "\n",
    "with open(dest_directory+'best_run.pkl', 'rb') as f:  \n",
    "    best_run = pickle.load(f)    \n",
    "'''\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(x_test, y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)\n",
    "best_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "dest_directory_temp =dest_directory + 'bestModel('+now.strftime(\"%m-%d %H.%M\")+\")\"\n",
    "if not os.path.exists(dest_directory_temp):\n",
    "      os.makedirs(dest_directory_temp)\n",
    "best_model.save(dest_directory_temp + '/best_model.bak')\n",
    "\n",
    "with open(dest_directory_temp + '/best_run.pkl', 'wb') as f:  \n",
    "    pickle.dump(best_run, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.best_trial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
