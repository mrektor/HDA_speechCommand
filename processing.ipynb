{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "## save variables\n",
    "import pickle\n",
    "## folder names\n",
    "from glob import glob\n",
    "## standard libraries\n",
    "import numpy as np\n",
    "## division for train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "##\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from keras.activations import softmax\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, Convolution2D, MaxPooling2D, AveragePooling2D\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import sys\n",
    "#!{sys.executable} -m pip install hyperas --user\n",
    "#!{sys.executable} -m pip install networkx==1.11 --user\n",
    "#!{sys.executable} -m pip install --upgrade keras --user\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "from hyperopt import Trials, STATUS_OK, tpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KCenter Functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def plotConfusionMatrix(predictions, true_labels, labels):\n",
    "    k = true_labels.shape[1]\n",
    "    n = true_labels.shape[0]\n",
    "    confusion_matrix = np.zeros((k,k))\n",
    "\n",
    "    for l in range(n):\n",
    "        decision = np.zeros(k)\n",
    "        j = np.argmax(predictions[l])\n",
    "        decision[j] = 1\n",
    "        i = np.argmax(true_labels[l])\n",
    "        confusion_matrix[i,j] +=1\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(confusion_matrix)\n",
    "    plt.title('Confusion matrix of the classifier')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + labels)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "def kcenter(data, k):\n",
    "    if k == data.shape[0]:\n",
    "        return data\n",
    "    '''\n",
    "    input:\n",
    "    data = numpy ndarray of shape (n_examples, a, b, c)\n",
    "    k : integer\n",
    "\n",
    "    output:\n",
    "    data reduced -> the new shape will be (k, a, b, c)\n",
    "    '''\n",
    "\n",
    "    n_examples = data.shape[0]\n",
    "\n",
    "\n",
    "    P = np.reshape(data, (n_examples, -1))\n",
    "    P_minus_S = [p for p in P]\n",
    "    idx_rnd = np.random.randint(0, len(P)-1)\n",
    "    S = [P[idx_rnd]]\n",
    "    dist_near_center = [np.linalg.norm(P[i]-S[0]) for i in range(len(P))]\n",
    "\n",
    "    for i in range(k-1):\n",
    "        progbar(i, k-2, 50)\n",
    "        #if i%150==0 : print(\"progress: \" + str(i/(k-1)*100) + \" %\")\n",
    "        new_center_idx = max(enumerate(dist_near_center), key=lambda x: x[1])[0] # argmax operation\n",
    "        S.append(P[new_center_idx])\n",
    "\n",
    "    for j in range(len(P)):\n",
    "        if j != new_center_idx:\n",
    "            dist = np.linalg.norm(P[j]-S[-1])\n",
    "            if dist < dist_near_center[j]:\n",
    "                dist_near_center[j] = dist\n",
    "        else:\n",
    "            dist_near_center[j] = 0\n",
    "    print()\n",
    "\n",
    "    #numpy darray tranformation\n",
    "    data_vector_reduced = np.array(S)\n",
    "    #Shape back\n",
    "    dimensions = (k, data.shape[1], data.shape[2])\n",
    "    data_back = np.reshape(data_vector_reduced, dimensions)\n",
    "\n",
    "    return data_back\n",
    "\n",
    "\n",
    "def kcenter_idx(data, k):\n",
    "    if k == data.shape[0]:\n",
    "        return range(k)\n",
    "    '''\n",
    "    input:\n",
    "        data = numpy ndarray of shape (n_examples, a, b, c)\n",
    "        k : integer\n",
    "        \n",
    "    output:\n",
    "        data reduced -> the new shape will be (k, a, b, c)\n",
    "    '''\n",
    "    \n",
    "    n_examples = data.shape[0]\n",
    "    \n",
    "    \n",
    "    P = np.reshape(data, (n_examples, -1))\n",
    "    P_minus_S = [p for p in P]\n",
    "    idx_rnd = np.random.randint(0, len(P)-1)\n",
    "    center_idx = [idx_rnd]\n",
    "    S = [P[idx_rnd]]\n",
    "    dist_near_center = [np.linalg.norm(P[i]-S[0]) for i in range(len(P))]\n",
    "\n",
    "    for i in range(k-1):\n",
    "        progbar(i, k-2, 50)\n",
    "        #if i%150==0 : print(\"progress: \" + str(i/(k-1)*100) + \" %\")\n",
    "        new_center_idx = max(enumerate(dist_near_center), key=lambda x: x[1])[0] # argmax operation\n",
    "        S.append(P[new_center_idx])\n",
    "        center_idx.append(new_center_idx)\n",
    "        \n",
    "        for j in range(len(P)):\n",
    "            if j != new_center_idx:\n",
    "                dist = np.linalg.norm(P[j]-S[-1])\n",
    "                if dist < dist_near_center[j]:\n",
    "                    dist_near_center[j] = dist\n",
    "            else:\n",
    "                dist_near_center[j] = 0\n",
    "    print()\n",
    "    \n",
    "    #numpy darray tranformation\n",
    "    data_vector_reduced = np.array(S)\n",
    "    #Shape back\n",
    "    dimensions = (k, data.shape[1], data.shape[2])\n",
    "    data_back = np.reshape(data_vector_reduced, dimensions)\n",
    "    \n",
    "    return center_idx\n",
    "\n",
    "\n",
    "def progbar(curr, total, full_progbar):\n",
    "    frac = curr/total\n",
    "    filled_progbar = round(frac*full_progbar)\n",
    "    print('\\r', '='*filled_progbar + '-'*(full_progbar-filled_progbar), '[{:>7.2%}]'.format(frac), end='')\n",
    "\n",
    "def train_test_creator(dic, down_size = 1, test_size = 0.2):\n",
    "    X = []\n",
    "    Y = []\n",
    "    if type(dic)==dict:\n",
    "        length = len(dic)\n",
    "        for count, key in enumerate(dic):\n",
    "            tmp = dic[key]\n",
    "            tmp = tmp[kcenter_idx(tmp, round(tmp.shape[0]/down_size)),:,:]\n",
    "            label = np.array(count)\n",
    "            label = np.resize(label, (tmp.shape[0],1))\n",
    "            X.append(tmp)\n",
    "            Y.append(label)\n",
    "            #print(key, ' ', count)\n",
    "    else:\n",
    "        return -1\n",
    "    X = np.vstack(X)\n",
    "    Y = np.vstack(Y)\n",
    "    \n",
    "    #X = np.reshape(X, ( X.shape[0],1, X.shape[1], X.shape[2]))\n",
    "    Y = np_utils.to_categorical(Y, np.max(Y)+1)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size)\n",
    "    X_train = np.reshape(X_train, ( X_train.shape[0],1, X_train.shape[1], X_train.shape[2]))\n",
    "    X_test = np.reshape(X_test, ( X_test.shape[0], 1, X_test.shape[1], X_test.shape[2]))\n",
    "    return X_train, X_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No-K Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotConfusionMatrix(predictions, true_labels, labels):\n",
    "    k = true_labels.shape[1]\n",
    "    n = true_labels.shape[0]\n",
    "    confusion_matrix = np.zeros((k,k))\n",
    "\n",
    "    for l in range(n):\n",
    "        decision = np.zeros(k)\n",
    "        j = np.argmax(predictions[l])\n",
    "        decision[j] = 1\n",
    "        i = np.argmax(true_labels[l])\n",
    "        confusion_matrix[i,j] +=1\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(confusion_matrix)\n",
    "    plt.title('Confusion matrix of the classifier')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + labels)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def train_test_creator(dic, test_size = 0.2):\n",
    "    X = []\n",
    "    Y = []\n",
    "    if type(dic)==dict:\n",
    "        length = len(dic)\n",
    "        for count, key in enumerate(dic):\n",
    "            tmp = dic[key]\n",
    "            label = np.array(count)\n",
    "            label = np.resize(label, (tmp.shape[0],1))\n",
    "            X.append(tmp)\n",
    "            Y.append(label)\n",
    "            #print(key, ' ', count)\n",
    "    else:\n",
    "        return -1\n",
    "    X = np.vstack(X)\n",
    "    Y = np.vstack(Y)\n",
    "    \n",
    "    Y = np_utils.to_categorical(Y, np.max(Y)+1)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size)\n",
    "    X_train = np.reshape(X_train, ( X_train.shape[0],1, X_train.shape[1], X_train.shape[2]))\n",
    "    X_test = np.reshape(X_test, ( X_test.shape[0], 1, X_test.shape[1], X_test.shape[2]))\n",
    "    with open('variables/train_test_split.pkl', 'wb') as f:  \n",
    "        pickle.dump(X_train, f)\n",
    "        pickle.dump(y_train, f)\n",
    "        pickle.dump(X_test, f)\n",
    "        pickle.dump(y_test, f)    \n",
    "    return X_train, y_train, X_test, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('variables/rawDict.pkl', 'rb') as f:  \n",
    "    #rawDict = pickle.load(f)\n",
    "with open('variables/noiseDict.pkl', 'rb') as f:  \n",
    "    noiseDict = pickle.load(f)    \n",
    "with open('variables/mfccDict.pkl', 'rb') as f:  \n",
    "    mfccDict = pickle.load(f)\n",
    "with open('variables/fbankDict.pkl', 'rb') as f:  \n",
    "    fbankDict = pickle.load(f)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreKey = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", \"zero\",\n",
    "           \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
    "\n",
    "numbers = {'one', 'two', 'three','four','five','six','seven','eight','nine'}\n",
    "words = {\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", \"zero\"}\n",
    "# ecco, bravo.\n",
    "# ahah ma che buggata è sta barra che avanza\n",
    "#c'è di peggio dai...\n",
    "#ma sei sicuro che stia funzinoando bene?\n",
    "#No per niente è il tuo algoritmo ahah\n",
    "#allora perfetto\n",
    "\n",
    "used = coreKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scriviamo qui per il dataset k-center"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sottomultiplo = 10\n",
    "dic = {k: mfccDict[k] for k in mfccDict.keys() & words}\n",
    "X_train_k, X_test_k, y_train_k, y_test_k = train_test_creator(dic , down_size = sottomultiplo, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrazione dataset non k-center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = train_test_creator({k: mfccDict[k] for k in mfccDict.keys() & used })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A' mejo rete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#def soft()\n",
    "cnn = Sequential()\n",
    "\n",
    "cnn.add(Convolution2D(32, (5,4),  strides = (1,1), padding=\"valid\",  activation=\"softplus\",\n",
    "                      input_shape=(1, x_train.shape[2], x_train.shape[3])))\n",
    "\n",
    "cnn.add(MaxPooling2D(pool_size=(3,2)))\n",
    "cnn.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "cnn.add(Convolution2D(64, (4,2),  strides = (1,1), padding=\"valid\", activation=\"softplus\"))\n",
    "cnn.add(Convolution2D(128, (4,3),  strides = (1,1), padding=\"valid\", activation=\"softplus\"))\n",
    "\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "cnn.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "cnn.add(Flatten())\n",
    "\n",
    "cnn.add(Dense(80, activation=\"softplus\"))\n",
    "cnn.add(Dropout(0.6))\n",
    "\n",
    "#cnn.add(Dense(30, activation=\"sigmoid\"))\n",
    "#cnn.add(Dropout(0.35))\n",
    "\n",
    "cnn.add(Dense(y_train.shape[1], activation=\"softmax\"))\n",
    "\n",
    "# define optimizer and objective, compile cnn\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch = 40\n",
    "cnn.compile(loss=\"categorical_crossentropy\", optimizer=\"adamax\", metrics=['accuracy'])\n",
    "cnn.fit(X_train, y_train, epochs=epoch, validation_data=(x_test, y_test), batch_size=round(x_train.shape[0]/250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds = cnn.predict(x_test)\n",
    "plotConfusionMatrix(preds,y_test,list(used))\n",
    "loss, precision = cnn.evaluate(x_test,y_test)\n",
    "print (\"Precision: \", round(precision*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "dest_directory = 'model_backup/'\n",
    "if not os.path.exists(dest_directory):\n",
    "      os.makedirs(dest_directory)\n",
    "name = 'cnn.bak'\n",
    "cnn.save(dest_directory + name)\n",
    "\n",
    "#bak = load_model(dest_directory + name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking for hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    with open('variables/train_test_split.pkl', 'rb') as f: \n",
    "        x_train = pickle.load(f)\n",
    "        y_train = pickle.load(f)\n",
    "        x_test = pickle.load(f)\n",
    "        y_test = pickle.load(f) \n",
    "    return x_train, y_train, x_test, y_test \n",
    "\n",
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    cnn = Sequential()\n",
    "    input_shape = (1, x_train.shape[2], x_train.shape[3])\n",
    "    cnn.add(Convolution2D({{choice([16,32,64])}}, (5,4),  strides = (1,1), padding=\"valid\", input_shape=input_shape))\n",
    "    cnn.add(Activation(\"softplus\"))\n",
    "    cnn.add(MaxPooling2D(pool_size=(3,2)))\n",
    "    cnn.add(Dropout({{uniform(0, .5)}}))\n",
    "\n",
    "    cnn.add(Convolution2D({{choice([32,64,128])}}, (4,2),  strides = (1,1), padding=\"valid\"))\n",
    "    cnn.add(Activation(\"softplus\"))\n",
    "    cnn.add(Convolution2D({{choice([64,128,256])}}, (4,3),  strides = (1,1), padding=\"valid\"))\n",
    "    cnn.add(Activation(\"softplus\"))\n",
    "    cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    cnn.add(Dropout({{uniform(0, .5)}}))\n",
    "\n",
    "    cnn.add(Flatten())\n",
    "\n",
    "    cnn.add(Dense({{choice([40, 60, 80, 100])}}))\n",
    "    cnn.add(Activation(\"softplus\"))\n",
    "    cnn.add(Dropout({{uniform(0, 1)}}))\n",
    "            \n",
    "    if conditional({{choice(['three', 'four'])}}) == 'four':\n",
    "        cnn.add(Dense({{choice([20, 30, 40])}}))\n",
    "        cnn.add(Activation(\"softplus\"))\n",
    "        cnn.add(Dropout({{uniform(0, .5)}}))\n",
    "\n",
    "    cnn.add(Dense(y_train.shape[1]))\n",
    "    cnn.add(Activation({{choice(['softmax', 'sigmoid'])}}))\n",
    "    cnn.compile(loss=\"categorical_crossentropy\", optimizer=\"adamax\", metrics=['accuracy'])\n",
    "            \n",
    "    cnn.fit(x_train, y_train,\n",
    "              batch_size={{choice([128, 256])}},\n",
    "              epochs=5,\n",
    "              verbose=2,\n",
    "              validation_data=(x_test, y_test))\n",
    "    score, acc = cnn.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': cnn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pickle\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from glob import glob\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.decomposition import PCA\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import normalize\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.activations import softmax\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential, load_model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Activation, Flatten, Dropout, Convolution2D, MaxPooling2D, AveragePooling2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import SGD\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os.path\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Convolution2D': hp.choice('Convolution2D', [16,32,64]),\n",
      "        'Dropout': hp.uniform('Dropout', 0, .5),\n",
      "        'Convolution2D_1': hp.choice('Convolution2D_1', [32,64,128]),\n",
      "        'Convolution2D_2': hp.choice('Convolution2D_2', [64,128,256]),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, .5),\n",
      "        'Dense': hp.choice('Dense', [40, 60, 80, 100]),\n",
      "        'Dropout_2': hp.uniform('Dropout_2', 0, 1),\n",
      "        'conditional': hp.choice('conditional', ['three', 'four']),\n",
      "        'Dense_1': hp.choice('Dense_1', [20, 30, 40]),\n",
      "        'Dropout_3': hp.uniform('Dropout_3', 0, .5),\n",
      "        'Activation': hp.choice('Activation', ['softmax', 'sigmoid']),\n",
      "        'batch_size': hp.choice('batch_size', [128, 256]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: with open('variables/train_test_split.pkl', 'rb') as f: \n",
      "  3:     x_train = pickle.load(f)\n",
      "  4:     y_train = pickle.load(f)\n",
      "  5:     x_test = pickle.load(f)\n",
      "  6:     y_test = pickle.load(f) \n",
      "  7: \n",
      "  8: \n",
      "  9: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     cnn = Sequential()\n",
      "   4:     input_shape = (1, x_train.shape[2], x_train.shape[3])\n",
      "   5:     cnn.add(Convolution2D(space['Convolution2D'], (5,4),  strides = (1,1), padding=\"valid\", input_shape=input_shape))\n",
      "   6:     cnn.add(Activation(\"softplus\"))\n",
      "   7:     cnn.add(MaxPooling2D(pool_size=(3,2)))\n",
      "   8:     cnn.add(Dropout(space['Dropout']))\n",
      "   9: \n",
      "  10:     cnn.add(Convolution2D(space['Convolution2D_1'], (4,2),  strides = (1,1), padding=\"valid\"))\n",
      "  11:     cnn.add(Activation(\"softplus\"))\n",
      "  12:     cnn.add(Convolution2D(space['Convolution2D_2'], (4,3),  strides = (1,1), padding=\"valid\"))\n",
      "  13:     cnn.add(Activation(\"softplus\"))\n",
      "  14:     cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
      "  15:     cnn.add(Dropout(space['Dropout_1']))\n",
      "  16: \n",
      "  17:     cnn.add(Flatten())\n",
      "  18: \n",
      "  19:     cnn.add(Dense(space['Dense']))\n",
      "  20:     cnn.add(Activation(\"softplus\"))\n",
      "  21:     cnn.add(Dropout(space['Dropout_2']))\n",
      "  22:             \n",
      "  23:     if conditional(space['conditional']) == 'four':\n",
      "  24:         cnn.add(Dense(space['Dense_1']))\n",
      "  25:         cnn.add(Activation(\"softplus\"))\n",
      "  26:         cnn.add(Dropout(space['Dropout_3']))\n",
      "  27: \n",
      "  28:     cnn.add(Dense(y_train.shape[1]))\n",
      "  29:     cnn.add(Activation(space['Activation']))\n",
      "  30:     cnn.compile(loss=\"categorical_crossentropy\", optimizer=\"adamax\", metrics=['accuracy'])\n",
      "  31:             \n",
      "  32:     cnn.fit(x_train, y_train,\n",
      "  33:               batch_size=space['batch_size'],\n",
      "  34:               epochs=5,\n",
      "  35:               verbose=2,\n",
      "  36:               validation_data=(x_test, y_test))\n",
      "  37:     score, acc = cnn.evaluate(x_test, y_test, verbose=0)\n",
      "  38:     print('Test accuracy:', acc)\n",
      "  39:     return {'loss': -acc, 'status': STATUS_OK, 'model': cnn}\n",
      "  40: \n",
      "Train on 37878 samples, validate on 9470 samples\n",
      "Epoch 1/5\n",
      " - 3s - loss: 3.0492 - acc: 0.0484 - val_loss: 2.9959 - val_acc: 0.0561\n",
      "Epoch 2/5\n",
      " - 3s - loss: 2.9958 - acc: 0.0501 - val_loss: 2.9959 - val_acc: 0.0469\n",
      "Epoch 3/5\n",
      " - 3s - loss: 2.9957 - acc: 0.0515 - val_loss: 2.9960 - val_acc: 0.0463\n",
      "Epoch 4/5\n",
      " - 3s - loss: 2.9957 - acc: 0.0512 - val_loss: 2.9960 - val_acc: 0.0463\n",
      "Epoch 5/5\n",
      " - 3s - loss: 2.9957 - acc: 0.0520 - val_loss: 2.9961 - val_acc: 0.0463\n",
      "Test accuracy: 0.04625131996877594\n",
      "Train on 37878 samples, validate on 9470 samples\n",
      "Epoch 1/5\n",
      " - 3s - loss: 3.0677 - acc: 0.0535 - val_loss: 2.9752 - val_acc: 0.0744\n",
      "Epoch 2/5\n",
      " - 3s - loss: 2.9458 - acc: 0.0696 - val_loss: 2.8273 - val_acc: 0.1580\n",
      "Epoch 3/5\n",
      " - 3s - loss: 2.8394 - acc: 0.0947 - val_loss: 2.6385 - val_acc: 0.2245\n",
      "Epoch 4/5\n",
      " - 3s - loss: 2.7314 - acc: 0.1228 - val_loss: 2.5127 - val_acc: 0.2971\n",
      "Epoch 5/5\n",
      " - 3s - loss: 2.6117 - acc: 0.1550 - val_loss: 2.3354 - val_acc: 0.3931\n",
      "Test accuracy: 0.3931362196787358\n",
      "Train on 37878 samples, validate on 9470 samples\n",
      "Epoch 1/5\n",
      " - 4s - loss: 3.1375 - acc: 0.0511 - val_loss: 3.0036 - val_acc: 0.0463\n",
      "Epoch 2/5\n",
      " - 4s - loss: 3.0085 - acc: 0.0494 - val_loss: 2.9972 - val_acc: 0.0463\n",
      "Epoch 3/5\n",
      " - 4s - loss: 3.0047 - acc: 0.0492 - val_loss: 2.9964 - val_acc: 0.0478\n",
      "Epoch 4/5\n",
      " - 4s - loss: 3.0036 - acc: 0.0505 - val_loss: 2.9962 - val_acc: 0.0463\n",
      "Epoch 5/5\n",
      " - 4s - loss: 3.0028 - acc: 0.0495 - val_loss: 2.9963 - val_acc: 0.0491\n",
      "Test accuracy: 0.04910242872778818\n",
      "Train on 37878 samples, validate on 9470 samples\n",
      "Epoch 1/5\n",
      " - 4s - loss: 3.0132 - acc: 0.0535 - val_loss: 2.9632 - val_acc: 0.0864\n",
      "Epoch 2/5\n",
      " - 3s - loss: 2.8979 - acc: 0.0967 - val_loss: 2.6969 - val_acc: 0.1917\n",
      "Epoch 3/5\n",
      " - 3s - loss: 2.6831 - acc: 0.1565 - val_loss: 2.4385 - val_acc: 0.2576\n",
      "Epoch 4/5\n",
      " - 3s - loss: 2.4881 - acc: 0.1949 - val_loss: 2.1754 - val_acc: 0.2816\n",
      "Epoch 5/5\n",
      " - 3s - loss: 2.3058 - acc: 0.2370 - val_loss: 1.9527 - val_acc: 0.3863\n",
      "Test accuracy: 0.386272439288237\n",
      "Train on 37878 samples, validate on 9470 samples\n",
      "Epoch 1/5\n",
      " - 5s - loss: 2.9927 - acc: 0.0854 - val_loss: 2.4739 - val_acc: 0.2738\n",
      "Epoch 2/5\n",
      " - 4s - loss: 2.3985 - acc: 0.2396 - val_loss: 1.7199 - val_acc: 0.5108\n",
      "Epoch 3/5\n",
      " - 4s - loss: 1.8971 - acc: 0.3954 - val_loss: 1.2126 - val_acc: 0.6795\n",
      "Epoch 4/5\n",
      " - 4s - loss: 1.5142 - acc: 0.5178 - val_loss: 0.8917 - val_acc: 0.7618\n",
      "Epoch 5/5\n",
      " - 4s - loss: 1.2576 - acc: 0.6047 - val_loss: 0.7017 - val_acc: 0.8036\n",
      "Test accuracy: 0.803590285047936\n"
     ]
    }
   ],
   "source": [
    "#from keras.layers.core import Dense, Dropout, Activation\n",
    "best_run, best_model = optim.minimize(model=create_model, \n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalutation of best performing model:\n",
      "9470/9470 [==============================] - 1s 82us/step\n",
      "[0.7017340095519012, 0.803590285047936]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'Activation': 0, 'Convolution2D': 1, 'Convolution2D_1': 1, 'Convolution2D_2': 1, 'Dense': 0, 'Dense_1': 2, 'Dropout': 0.06874772122693129, 'Dropout_1': 0.293803364162271, 'Dropout_2': 0.3746350041674067, 'Dropout_3': 0.2919157826643053, 'batch_size': 0, 'conditional': 1}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_28 (Conv2D)           (None, 32, 95, 10)        672       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 32, 95, 10)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 32, 31, 5)         0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 32, 31, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 64, 28, 4)         16448     \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 64, 28, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 128, 25, 2)        98432     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 128, 25, 2)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 128, 12, 1)        0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 128, 12, 1)        0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 40)                61480     \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 40)                1640      \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 179,492\n",
      "Trainable params: 179,492\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(x_test, y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da creare algoritmo che traduce il dizionario in rete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
